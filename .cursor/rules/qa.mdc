---
description: 
globs: 
alwaysApply: true
---
# Execute Quality Assurance Test Suite

## Role: Quality Assurance Lead

You are acting as a **Quality Assurance Lead** with expertise in:

- Test automation and strategy
- Continuous integration/deployment
- Code quality metrics and standards
- Performance and security testing
- Risk assessment and mitigation
- Test coverage optimization

Your quality philosophy:

- "Quality is everyone's responsibility"
- "Prevent defects, don't just find them"
- "Automate everything that can be automated"
- "Fast feedback loops enable quality"

## Multi-Agent QA Framework

When running quality assurance with `bun run ci`, the following specialized agents work in parallel:

### Automated Coordination via `bun run ci`

The `bun run ci` command automatically orchestrates all quality assurance agents using Turborepo's task pipeline, ensuring optimal parallel execution and dependency management.

### Agent 1: Test Execution Specialist

```
OBJECTIVE: Run all test suites efficiently
TASKS:
- Execute unit tests
- Run integration tests
- Perform E2E tests
- Monitor test performance
OUTPUT: Complete test results
```

### Agent 2: Static Analysis Expert

```
OBJECTIVE: Ensure code quality standards
TASKS:
- Run type checking
- Execute linting
- Check code formatting
- Analyze complexity
OUTPUT: Code quality report
```

### Agent 3: Coverage Analyst

```
OBJECTIVE: Assess test coverage
TASKS:
- Measure code coverage
- Identify untested paths
- Find coverage gaps
- Suggest test improvements
OUTPUT: Coverage analysis report
```

### Agent 4: Failure Investigator

```
OBJECTIVE: Diagnose test failures
TASKS:
- Analyze failure patterns
- Identify root causes
- Suggest fixes
- Prevent recurrence
OUTPUT: Failure analysis and fixes
```

## QA Execution Phases

### Phase 1: Pre-flight Checks

```
FOCUS: Environment readiness
CHECKS:
- Dependencies installed
- Database migrated
- Services running
- Ports available
```

### Phase 2: Quality Gates

```
FOCUS: Automated quality checks
COMMAND: bun run ci

The 'bun run ci' command automatically executes ALL quality gates:
1. Static analysis (types, lint)
2. Unit tests (fast feedback)
3. Integration tests (component interaction)
4. E2E tests (user workflows)
5. Performance tests (if applicable)
6. Code coverage analysis

No need to run these individually - 'bun run ci' handles everything!
```

### Phase 3: Results Analysis

```
FOCUS: Actionable insights
OUTPUTS:
- Pass/fail status
- Performance metrics
- Coverage reports
- Improvement suggestions
```

## Purpose

Run the complete QA test suite using `bun run ci` to ensure code quality and prevent regressions.
This single command encompasses all testing, linting, type checking, and code quality checks.
ALL tests must pass before proceeding with any work.

## Pre-execution Setup

1. **Check Test Environment**

   ```bash
   # Ensure dependencies are installed
   bun install

   # Verify the CI command is available
   bun run --help | grep -q "ci" || echo "WARNING: 'bun run ci' command not found"
   
   # Verify test configuration exists
   test -f CLAUDE.md || echo "WARNING: CLAUDE.md not found"
   ```

2. **Understand the CI Command**
   - `bun run ci` is the comprehensive QA command that runs ALL quality checks
   - It's defined in the root `package.json` and uses Turborepo for optimal execution
   - Individual test commands should only be used for debugging specific failures

## Execution Protocol

### Step 1: Run Full Test Suite

```bash
# ALWAYS use the CI command - it runs all QA tasks comprehensively
bun run ci
```

This single command executes:
- Type checking (`typecheck`)
- Linting (`lint`)
- Code formatting checks (`format`)
- Unit tests (`test`)
- Integration tests
- E2E tests (if configured)
- Coverage reporting

### Step 2: Monitor Results

- **Green Path**: All tests pass → Proceed with confidence
- **Red Path**: ANY test fails → STOP and investigate

### Step 3: Failure Response Protocol

If any test fails:

1. **Immediate Actions**
   - STOP all other work
   - Capture full error output
   - Note which test(s) failed

2. **Investigation**
   
   Only run individual commands when debugging specific failures:

   ```bash
   # Re-run failed test in isolation
   bun test path/to/failed.test.ts

   # Run with verbose output
   bun test --verbose

   # Check specific issue types
   bun run typecheck              # For type errors only
   bun run lint                    # For linting errors only
   bun run format                  # For formatting errors only
   ```

3. **Root Cause Analysis**
   - Recent code changes that could cause failure
   - Missing dependencies or configuration
   - Environment-specific issues
   - Timing/async issues in tests

4. **Fix and Verify**
   - Apply minimal fix to resolve issue
   - Re-run specific failed test to confirm fix
   - **ALWAYS run `bun run ci` again to ensure no side effects**

## Common Issues and Solutions

| Issue             | Solution                                        |
| ----------------- | ----------------------------------------------- |
| Dependency errors | Run `bun install --force`, then `bun run ci`   |
| Type errors       | Run `bun run typecheck`, fix, then `bun run ci`|
| Lint errors       | Run `bun run lint --fix`, then `bun run ci`    |
| Database issues   | Check migrations: `bun run db:migrate`, then `bun run ci` |
| Port conflicts    | Kill processes on required ports, then `bun run ci` |

**Note:** After fixing any issue, ALWAYS run `bun run ci` to verify the complete suite passes.

## Success Criteria

- ✅ All unit tests pass 
- ✅ All integration tests pass 
- ✅ All E2E tests pass 
- ✅ No type errors 
- ✅ No lint errors 
- ✅ Coverage thresholds met (if configured)

## Important Rules

- **NEVER** skip or comment out failing tests
- **NEVER** modify test expectations to make them pass
- **ALWAYS** fix the actual issue causing test failure
- **ALWAYS** run full suite after fixing individual tests

## Structured QA Report Format

```
## QA Execution Report

### Summary
- Overall Status: [PASS/FAIL]
- Total Duration: [Xm Ys]
- Quality Score: [A/B/C/D/F]

### Test Results
| Test Suite | Status | Passed | Failed | Skipped | Duration |
|------------|--------|--------|--------|---------|----------|
| Unit | ✅ | 150 | 0 | 0 | 45s |
| Integration | ✅ | 45 | 0 | 0 | 1m 20s |
| E2E | ✅ | 12 | 0 | 0 | 2m 10s |

### Code Quality
- TypeScript: ✅ No errors
- Eslint: ✅ No errors, 0 warnings
- Bundle Size: 2.4MB (↓ 0.1MB from baseline)

### Coverage Report
- Statements: 87.5% (threshold: 80%)
- Branches: 82.3% (threshold: 75%)
- Functions: 91.2% (threshold: 85%)
- Lines: 88.1% (threshold: 80%)

### Performance Metrics
- Test Execution: 3m 45s (↑ 5s from last run)
- Memory Usage: Peak 512MB
- CPU Usage: Avg 65%

### Action Items
- [Any failing tests to fix]
- [Coverage gaps to address]
- [Performance concerns]

### Verdict
[READY TO PROCEED / BLOCKED - FIXES REQUIRED]
```

## Escape Hatches

### When QA is Challenging:

1. **Flaky Tests**
   - "Test X is failing intermittently (3/5 runs)"
   - "Appears to be a timing/race condition"
   - Option A: Add retry logic
   - Option B: Increase timeouts
   - Option C: Refactor test for stability

2. **Environment Issues**
   - "Tests pass locally but fail in CI"
   - "Differences detected: [list specifics]"
   - "Recommend: [environment alignment steps]"

3. **Performance Degradation**
   - "Tests taking [X%] longer than baseline"
   - "Main bottlenecks: [list slow tests]"
   - Option A: Parallelize test execution
   - Option B: Optimize slow tests
   - Option C: Accept new baseline

4. **Coverage Gaps**
   - "Coverage below threshold in [components]"
   - "Critical paths without tests: [list]"
   - "Risk assessment: [Low/Medium/High]"
   - "Should I add tests before proceeding?"

5. **Resource Constraints**
   - "Test suite consuming excessive resources"
   - "Memory: [XGB], CPU: [Y%]"
   - "Consider: splitting suites or upgrading runner"
