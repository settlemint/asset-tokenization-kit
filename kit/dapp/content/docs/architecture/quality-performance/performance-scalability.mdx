---
title: Performance
description:
  Performance targets, optimization strategies, and scalability architecture for
  enterprise-grade asset tokenization
pageTitle: Performance and scalability - Enterprise-grade optimization strategy
tags: [architecture, performance, scalability, optimization, caching, concept]
---

The Asset Tokenization Kit is architected for institutional deployment where
sub-second response times, predictable throughput, and horizontal scalability
are non-negotiable. This page documents the performance targets, optimization
strategies, and scalability patterns that enable ATK to handle production
workloads without the degradation that plagues bolt-on tokenization stacks.

## Performance philosophy

Traditional tokenization platforms fail under load because they treat
performance as an afterthought—retrofitting caching layers onto slow databases,
adding CDNs in front of unoptimized frontends, and hoping blockchain delays
don't cascade into user-facing timeouts. ATK takes the opposite approach:
performance is a first-class architectural constraint, not a bolt-on
optimization.

Every component has explicit performance budgets enforced through automated
testing. The frontend measures First Contentful Paint and Time to Interactive.
The API tracks P95 response times per procedure. Smart contracts target gas cost
ceilings. The subgraph monitors indexing lag. When a metric drifts outside its
budget, the CI pipeline fails—forcing fixes before the regression reaches
production.

## Performance targets by component

These targets reflect real-world institutional requirements gathered from
capital markets deployments. They are measured in CI and production monitoring.

| Component          | Metric                     | Target       | Why It Matters                              |
| ------------------ | -------------------------- | ------------ | ------------------------------------------- |
| **Frontend**       | First Contentful Paint     | &lt;1.5s     | User engagement, perceived performance      |
| **Frontend**       | Time to Interactive        | &lt;2.5s     | Usability, task completion rates            |
| **Frontend**       | Largest Contentful Paint   | &lt;2.5s     | Core Web Vitals, SEO ranking                |
| **API Procedures** | P50 response time          | &lt;100ms    | Responsive UX, real-time data updates       |
| **API Procedures** | P95 response time          | &lt;300ms    | Consistent experience under load            |
| **API Procedures** | P99 response time          | &lt;500ms    | Tail latency management                     |
| **Database**       | Query response time        | &lt;50ms     | API procedure speed, data retrieval         |
| **Database**       | Connection pool saturation | &lt;80%      | Scalability headroom, connection management |
| **Database**       | Uptime                     | 99.95%       | Reliability, data availability              |
| **Blockchain**     | Transaction confirmation   | &lt;3s       | Settlement speed, finality perception       |
| **Blockchain**     | Average gas cost per tx    | &lt;150,000  | Transaction economics, cost predictability  |
| **Blockchain**     | Peak gas cost per tx       | &lt;300,000  | Worst-case cost ceiling                     |
| **Indexer**        | Event processing latency   | &lt;10s      | Data freshness, UI synchronization          |
| **Indexer**        | Query response time        | &lt;200ms    | GraphQL query performance                   |
| **Subgraph**       | Indexing lag (blocks)      | &lt;5 blocks | Near real-time data, synchronization        |
| **Cache (Redis)**  | Cache hit rate             | &gt;85%      | Reduced database load, faster responses     |
| **Cache (Redis)**  | Operation latency          | &lt;5ms      | In-memory performance, cache effectiveness  |

## Frontend performance

The dApp frontend is built with TanStack Start, React, and Vite with aggressive
optimization strategies that keep bundle sizes small and initial loads fast.

### Build-time optimizations

**Code splitting and lazy loading** - The Vite build configuration uses manual
chunk splitting to separate vendor libraries by usage pattern. Core dependencies
(React, TanStack Router, TanStack Query) bundle into a shared chunk loaded on
every route. Feature-specific libraries (Recharts for analytics, Mermaid for
documentation, TanStack Table for data grids) split into route-level chunks
loaded only when needed.

**Tree shaking and dead code elimination** - ES modules enable Vite's
tree-shaking to strip unused exports. The build targets ES2023, allowing modern
syntax without transpilation bloat. Unused CSS classes are purged by Tailwind's
JIT compiler.

**Asset optimization** - Images use modern formats (WebP, AVIF) with lazy
loading via `loading="lazy"`. SVG icons inline when small, external when large.
Font subsetting reduces WOFF2 files to only the glyphs used in the UI.

**Pre-bundling dependencies** - Vite's `optimizeDeps.include` configuration
pre-bundles frequently accessed libraries (`@orpc/client`, `viem`, `zod`,
`date-fns`) during dev server startup. This eliminates cold-start
re-optimization and improves hot module replacement (HMR) speed.

### Runtime optimizations

**React Compiler** - The frontend uses the experimental React Compiler to
automatically memoize components and hooks, eliminating manual `useMemo` and
`useCallback` wrappers. This reduces unnecessary re-renders and improves runtime
performance without developer overhead.

**TanStack Query caching** - API responses cache in TanStack Query with
configurable stale times and cache invalidation strategies. Token balances cache
for 30 seconds (frequent updates expected). Asset metadata caches for 5 minutes
(relatively stable). Identity verification results cache for 10 minutes (changes
infrequently).

**Broadcast synchronization** - Multiple browser tabs synchronize cache state
via the experimental `@tanstack/query-broadcast-client`. When one tab
invalidates a query (e.g., after minting tokens), all other tabs refetch
automatically. This prevents stale data without forcing constant background
polling.

**Virtualized lists** - Large datasets (token holder tables, transaction
histories) use virtual scrolling to render only visible rows. A 10,000-row table
renders 20 DOM nodes instead of 10,000, keeping scroll performance smooth.

**Prefetching and preloading** - TanStack Router prefetches route code and data
when hovering over links. Critical assets (fonts, above-the-fold images) use
`<link rel="preload">` to prioritize loading.

### Performance monitoring

**Core Web Vitals tracking** - The frontend measures Largest Contentful Paint
(LCP), First Input Delay (FID), and Cumulative Layout Shift (CLS). These metrics
feed into production monitoring dashboards and trigger alerts when thresholds
degrade.

**Bundle size budgets** - CI enforces bundle size limits per chunk. The main
bundle stays under 200KB gzipped. Vendor chunks target 150KB each. Oversized
bundles block deployment until developers analyze and reduce them.

**Lighthouse CI integration** - Automated Lighthouse audits run on every pull
request. Regressions in performance, accessibility, or best practices scores
block merging until resolved.

## API performance

The ORPC-based API layer prioritizes low latency and high throughput through
careful architectural choices.

### Type-safe procedure routing

**Zero-overhead abstraction** - ORPC compiles TypeScript procedure definitions
into minimal runtime code. Unlike REST frameworks with heavyweight routing logic
and JSON schema validation, ORPC procedures execute with negligible overhead.
Request validation uses Zod schemas compiled at build time, avoiding runtime
parsing penalties.

**Procedure batching** - Multiple client requests batch into a single HTTP
round-trip when possible. Fetching an asset's metadata, holder count, and
compliance status in one batch reduces latency from 3 × 100ms (serialized) to 1
× 120ms (parallel execution with network overhead).

**Streaming responses** - Long-running operations (bulk airdrops, compliance
scans) stream progress updates to the client rather than blocking until
completion. Users see real-time feedback, and server memory pressure remains
constant instead of accumulating state.

### Database query optimization

**Connection pooling** - PostgreSQL connections pool via Drizzle ORM with a
target pool size of 20 connections. Queries reuse idle connections instead of
incurring connection establishment overhead (typically 50-100ms). Pool
saturation monitoring alerts when usage exceeds 80%, triggering horizontal
scaling.

**Query optimization patterns** - Database queries follow strict optimization
patterns. Always project only needed columns (avoid `SELECT *`). Use indexes on
foreign keys and frequently filtered columns. Paginate large result sets with
cursor-based navigation. Batch related queries using `IN` clauses instead of N+1
queries.

**Read replicas for reporting** - Heavy read workloads (analytics dashboards,
holder reports) route to read replicas rather than the primary database. This
offloads the primary for write-heavy operations (minting, transfers, compliance
updates) and prevents read contention from degrading write performance.

**Prepared statements** - Drizzle ORM generates parameterized queries that
PostgreSQL prepares and caches. Repeated queries (checking if an address is
verified, fetching token metadata) execute faster after the first invocation
because the query plan is cached.

### Caching strategies

**Redis for session and hot data** - Better Auth session tokens cache in Redis
for sub-millisecond authentication checks. Frequently accessed data (user
profiles, token summaries) cache with short TTLs (30-120 seconds) to reduce
database load during bursts.

**Stale-while-revalidate pattern** - API responses include
`Cache-Control: stale-while-revalidate=30` headers. Clients receive cached data
immediately while the API asynchronously refreshes the cache in the background.
This balances freshness with speed—users never wait for slow database queries.

**Cache invalidation coordination** - When blockchain events modify state
(mints, burns, transfers), the subgraph triggers webhook notifications to the
API. The API invalidates relevant cache entries (token holder lists, balance
aggregates) and broadcasts invalidation messages to connected clients via
TanStack Query's broadcast channel.

### Rate limiting and load shedding

**Per-user rate limits** - Authenticated procedures enforce rate limits (100
requests per minute per user). Unauthenticated endpoints (public asset data) use
IP-based limits (500 requests per minute). Rate limiters use Redis sliding
windows to track usage across API instances.

**Graceful degradation** - Under extreme load, non-critical features degrade
before core functionality. Analytics dashboards serve cached data. Admin panels
delay non-urgent updates. Asset minting and transfers remain fast by shedding
low-priority traffic.

## Smart contract performance

Gas costs directly impact transaction economics. ATK optimizes contracts to
minimize gas consumption while maintaining security and compliance.

### Gas optimization strategies

**Storage layout optimization** - Solidity structs pack tightly to minimize
storage slots. A `uint32` timestamp, `uint16` country code, and `bool` flag fit
into a single 32-byte slot (saving 15,000 gas per write) instead of spanning
three slots (45,000 gas).

**Batch operations** - Minting tokens to multiple recipients in one transaction
costs 150,000 gas for the first recipient and 50,000 gas for each additional
recipient. Ten individual transactions cost 1,500,000 gas; one batch transaction
costs 600,000 gas. Batch operations reduce per-recipient costs by 60%.

**Event optimization** - Events cost far less than storage writes (8 gas per
byte vs 20,000 gas per word). Contracts emit detailed events for off-chain
indexing rather than storing metadata on-chain. The subgraph reconstructs state
from events, keeping gas costs low while preserving auditability.

**Unchecked arithmetic** - Math operations in contexts where overflow is
impossible (incrementing a counter guaranteed to stay under 2^32) use
`unchecked` blocks to skip redundant safety checks. This saves approximately 100
gas per operation.

**Custom errors** - Contracts use custom errors
(`error InsufficientBalance(address account)`) instead of revert strings. Custom
errors cost 1,500 gas; revert strings cost 8,000 gas. This optimization applies
to every failure case.

### Compliance module efficiency

**Modular validation** - Compliance checks run only the configured modules for
each token. A bond with country restrictions and investor caps skips identity
verification modules, saving 30,000 gas per transfer. An equity token with full
KYC runs all modules, prioritizing security over gas savings.

**Bitmap flags for eligibility** - Country allow/block lists use bitmaps instead
of mappings. Checking if a country is allowed costs 2,100 gas (one storage slot
read); a mapping check costs 2,100 gas plus 20,000 gas for a cold slot access.
Bitmaps reduce average validation costs by 40%.

**Signature verification caching** - Identity claim signatures verify once and
cache the result. Subsequent transfers by the same address skip re-verification
if the claim hasn't expired. This reduces the most expensive operation (ECDSA
signature recovery: 3,000 gas) to a cheap storage read (2,100 gas).

### Upgrade safety and gas costs

**UUPS proxy pattern** - Contracts use Universal Upgradeable Proxy Standard
(UUPS) for upgradeability. UUPS stores upgrade logic in the implementation
contract, not the proxy. This saves 15,000 gas per call compared to Transparent
Proxy patterns where the proxy checks `msg.sender` on every invocation.

**Minimal proxy clones** - New token deployments use EIP-1167 minimal proxy
clones pointing to a shared implementation. Deploying a clone costs 45,000 gas;
deploying a full contract costs 1,200,000 gas. The factory pattern reduces
deployment costs by 96%.

## Blockchain and indexing performance

Blockchain performance depends on the underlying network, but ATK optimizes
interaction patterns to minimize latency.

### Transaction batching

**Multicall aggregation** - Reading multiple contract values (token balance,
allowance, metadata) batches into a single multicall transaction. Instead of
three JSON-RPC calls with 100ms latency each (300ms total), one multicall
completes in 110ms (100ms latency + 10ms execution).

**Sequential transaction pipelining** - When submitting dependent transactions
(approve then transfer), the frontend estimates gas, signs both transactions,
and submits them immediately rather than waiting for the first to confirm. Block
builders include both in the same block when possible, reducing time-to-finality
from 6 seconds (two blocks) to 3 seconds (one block).

### Subgraph indexing speed

**Event-driven architecture** - TheGraph subgraph subscribes to smart contract
events rather than polling blocks. New events trigger indexing within 2-3
seconds (one block time + processing). Polling architectures incur 10-30 second
delays due to batch intervals.

**Schema denormalization** - The subgraph schema denormalizes relationships to
reduce query complexity. Token holder balances store both the `tokenId` and
`tokenName` instead of requiring a join to the token entity. This trades storage
space for query speed—acceptable because indexers handle storage well.

**Parallel processing** - TheGraph node configuration enables parallel block
processing when events are independent. Minting one token and transferring
another token process concurrently, doubling indexing throughput.

### GraphQL query optimization

**Query cost limiting** - Clients paginate large result sets (max 1,000 items
per query) to prevent unbounded queries from exhausting subgraph resources. The
UI implements infinite scroll with cursor-based pagination rather than loading
all token holders at once.

**Field selection discipline** - Queries request only needed fields. Fetching a
token holder list requests `address` and `balance`, not the full `Token` entity
with metadata. This reduces response payload size from 500KB to 50KB and query
execution time from 200ms to 50ms.

## Database performance

PostgreSQL serves as the source of truth for application state and user data.
Its performance directly impacts API responsiveness.

### Indexing strategy

**Composite indexes** - Queries filtering by multiple columns (e.g.,
`WHERE token_id = $1 AND status = 'active'`) use composite indexes
`(token_id, status)` to avoid full table scans. Index-only scans complete in
milliseconds; table scans take seconds as tables grow.

**Partial indexes** - Indexes on filtered subsets (e.g.,
`CREATE INDEX ON transfers (created_at) WHERE status = 'pending'`) reduce index
size and improve write performance. A partial index on pending transfers is 1%
the size of a full index, accelerating both reads and writes.

**Covering indexes** - Indexes include all columns needed by a query to enable
index-only scans that never touch the table. Example:
`CREATE INDEX ON tokens (id) INCLUDE (name, symbol)` allows
`SELECT id, name, symbol FROM tokens WHERE id = $1` to complete from the index
alone.

### Connection management

**Connection pooling** - Drizzle ORM connects to PostgreSQL through PgBouncer in
transaction pooling mode. The pool maintains 20 persistent connections to
PostgreSQL while serving 200 concurrent application connections. Without
pooling, 200 concurrent connections would exhaust PostgreSQL's connection limit
and degrade performance.

**Statement timeouts** - All queries have a 5-second timeout. Queries exceeding
this limit are canceled and logged. This prevents slow queries (unindexed full
table scans) from blocking connection slots and cascading into API timeouts.

**Idle connection reaping** - Connections idle for more than 10 minutes close
automatically to free resources. This prevents connection leaks from keeping
database resources tied up.

### Maintenance and vacuuming

**Auto-vacuum tuning** - PostgreSQL auto-vacuum runs with aggressive settings
(`autovacuum_vacuum_scale_factor = 0.05`) to reclaim space from deleted rows
quickly. Frequent small vacuums prevent bloat from accumulating and degrading
query performance.

**Analyze for query planning** - Auto-analyze updates table statistics after 5%
of rows change. Accurate statistics ensure the query planner chooses optimal
index strategies. Stale statistics cause the planner to pick slow sequential
scans instead of fast index scans.

**Reindexing schedule** - Indexes rebuild quarterly to reclaim space and
eliminate fragmentation. Fragmented indexes grow to 2-3× their optimal size,
slowing lookups and wasting I/O.

## Caching architecture

Redis powers session management and hot-path caching. Its sub-millisecond
latency keeps the application responsive even when the database is under load.

### Cache hierarchy

**L1: In-memory application cache** - TanStack Query caches API responses in the
browser. Cache hits return data in microseconds without any network round-trip.
Stale times (30 seconds to 5 minutes) balance freshness with performance.

**L2: Redis cache** - API procedures cache frequently accessed data (user
profiles, token metadata) in Redis with short TTLs (1-2 minutes). Cache hits
return data in 2-5ms instead of 50ms (database query).

**L3: Database query cache** - PostgreSQL's shared buffer cache keeps frequently
accessed pages in memory. Hot data (recent transfers, active token holders)
stays in cache, avoiding disk I/O.

### Cache invalidation patterns

**Time-based expiration** - Most cache entries expire after a fixed TTL. This
works well for data that changes predictably (token prices update every minute;
user profiles change rarely).

**Event-driven invalidation** - Blockchain events trigger cache invalidation.
When a token transfer occurs, the API invalidates cache entries for the sender's
balance, recipient's balance, and token holder count. This ensures the UI
reflects state changes within seconds.

**Versioned cache keys** - Cache keys include a version component
(`token_metadata:v2:${tokenId}`). When the metadata schema changes, incrementing
the version abandons old cache entries without explicit deletion. This prevents
stale data from persisting across deployments.

### Cache warming strategies

**Pre-warming on deployment** - After deploying new API instances, a startup
script pre-warms the cache by fetching the 100 most popular tokens and their
metadata. This prevents cache stampedes when users access the application
immediately after deployment.

**Background refresh for hot keys** - Keys accessed frequently (more than 100
times per minute) refresh automatically 30 seconds before expiration. This
ensures high-traffic data never expires, preventing temporary performance dips
during cache rehydration.

## Scalability architecture

ATK scales horizontally by adding more instances of stateless components. No
single bottleneck limits throughput.

### Horizontal scaling patterns

**Stateless API instances** - ORPC API servers hold no session state. Requests
route to any available instance via a load balancer. Scaling from 2 instances to
10 instances increases API throughput from 2,000 requests/second to 10,000
requests/second linearly.

**Read replica scaling** - Adding PostgreSQL read replicas increases read
capacity proportionally. Two replicas double read throughput. Replicas lag the
primary by less than 1 second, so queries tolerate eventual consistency.

**Redis clustering** - Redis runs in cluster mode with data sharded across 3
nodes. Each node handles a portion of the keyspace. Adding nodes increases cache
capacity and throughput proportionally.

### Load balancing strategies

**Round-robin for API instances** - HTTP requests distribute evenly across API
instances using round-robin load balancing. This maximizes throughput when all
instances have equal capacity.

**Least-connections for database** - Database queries route to the replica with
the fewest active connections. This prevents hot replicas from becoming
bottlenecks while cold replicas sit idle.

**Geographic routing for frontend** - The TanStack Start dApp deploys to
multiple regions. Users route to the nearest region based on geographic
proximity. This reduces latency from 200ms (cross-continent) to 20ms (same
continent).

### Auto-scaling policies

**CPU-based scaling for API** - Kubernetes HorizontalPodAutoscaler adds API pods
when average CPU utilization exceeds 70%. This prevents CPU saturation from
degrading response times during traffic spikes.

**Queue depth scaling for workers** - Background job workers scale based on
queue depth. When more than 100 jobs accumulate, Kubernetes spawns additional
worker pods to drain the queue faster.

**Connection pool saturation for database** - When connection pool utilization
exceeds 80% for more than 5 minutes, alerts trigger manual intervention to add
read replicas or increase pool size.

## Performance testing and monitoring

Automated performance testing runs in CI to catch regressions before deployment.

### Load testing

**K6 scenarios** - Load tests simulate realistic user patterns: browsing assets,
minting tokens, executing transfers. Tests ramp from 10 concurrent users to
1,000 users over 10 minutes. Response times must stay below P95 targets
throughout the ramp.

**Spike testing** - Spike tests validate behavior under sudden load increases
(e.g., 10,000 users appear instantly). The system should degrade gracefully
(slower responses) rather than failing catastrophically (timeouts, errors).

**Soak testing** - Soak tests run at moderate load (500 concurrent users) for 24
hours. This detects memory leaks, connection pool exhaustion, and other issues
that manifest only under sustained load.

### Contract gas benchmarking

**Foundry gas reports** - Every contract test measures gas consumption. CI
compares gas reports against the previous commit. Increases greater than 5%
block merging and require developer justification.

**Regression tracking** - A dashboard tracks gas costs over time. Unexpected
increases trigger investigations. Optimizations that reduce costs by more than
10% are celebrated and documented.

### Production monitoring

**Metrics collection** - OpenTelemetry instruments the API, collecting request
latency, error rates, and throughput. Metrics export to Prometheus for
aggregation and alerting.

**Distributed tracing** - Complex operations (minting tokens with compliance
checks) emit distributed traces. Traces reveal where latency accumulates: slow
database queries, network round-trips, or blockchain confirmations.

**Real User Monitoring (RUM)** - The frontend reports Core Web Vitals from real
user sessions. This reveals performance issues specific to certain browsers,
devices, or network conditions that synthetic tests miss.

## Optimization workflow

Performance optimization follows a disciplined process to ensure changes improve
metrics without introducing regressions.

### Measure first

**Establish baseline** - Before optimizing, measure current performance under
realistic load. Example: "Token holder list query takes 800ms at P95 with 10,000
holders."

**Identify bottlenecks** - Use profiling tools to find where time is spent.
Database queries? Network latency? Rendering? Focus optimization efforts on the
biggest contributor.

### Optimize targeted

**Implement change** - Apply one optimization at a time. Example: "Add composite
index on `(token_id, created_at)`."

**Measure impact** - Re-run performance tests. Document the improvement: "Token
holder list query now takes 120ms at P95, an 85% reduction."

**Compare cost vs benefit** - If the optimization added complexity (e.g.,
maintaining a denormalized table), ensure the performance gain justifies the
maintenance burden.

### Validate in production

**Canary deployments** - Deploy optimizations to 10% of production traffic
first. Monitor metrics for regressions (increased error rates, degraded
latency). If metrics improve, roll out to 100%.

**Rollback readiness** - Keep the previous version deployable. If an
optimization degrades performance unexpectedly, roll back within minutes rather
than scrambling to fix forward.

## Common performance anti-patterns

Avoid these mistakes that plague tokenization platforms:

- **Polling instead of push** - Don't poll the blockchain for events; use
  websockets or subgraph subscriptions for real-time updates
- **N+1 queries** - Load related data in batch queries, not one query per item
  in a loop
- **Unbounded pagination** - Always paginate large result sets; never return
  100,000 token holders in one response
- **Missing indexes** - Every foreign key and frequently filtered column should
  have an index
- **Synchronous external calls** - Don't block API responses waiting for
  third-party services; use background jobs with webhooks
- **Over-fetching GraphQL** - Request only the fields you need, not entire
  entity graphs
- **Premature optimization** - Measure before optimizing; don't guess where the
  bottleneck is
- **Ignoring cache invalidation** - Stale data erodes user trust; ensure caches
  invalidate promptly when state changes

## Conclusion

Performance is not a feature you add at the end; it's an architectural property
you design in from the beginning. ATK's performance targets are not
aspirational—they are enforced through automated testing and production
monitoring. When response times drift outside acceptable bounds, alerts fire and
teams respond.

This discipline ensures that ATK remains fast and scalable as adoption grows.
There are no hidden bottlenecks waiting to surface when you onboard your
10,000th investor or issue your 100th token. The architecture scales
horizontally, the database queries are optimized, and the smart contracts are
gas-efficient. Performance is predictable, measurable, and reliable.
