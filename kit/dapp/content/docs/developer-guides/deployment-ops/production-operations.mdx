---
title: Production operations
description: Production best practices, monitoring, and troubleshooting
---

This guide covers production best practices, CI/CD integration, monitoring, and
troubleshooting for the Asset Tokenization Kit.

## Production best practices

### Security hardening

1. **Use secrets management**

   Replace inline secrets with Kubernetes Secret references:

   ```yaml
   global:
     datastores:
       default:
         postgresql:
           existingSecret: "atk-postgres-secret"
           existingSecretKeys:
             password: password
   ```

   Create the secret separately:

   ```bash
   kubectl create secret generic atk-postgres-secret \
     --from-literal=password='YOUR_SECURE_PASSWORD' \
     -n atk
   ```

2. **Enable TLS termination**

   Configure ingress with TLS certificates:

   ```yaml
   dapp:
     ingress:
       enabled: true
       ingressClassName: "nginx"
       tls:
         - secretName: dapp-tls
           hosts:
             - dapp.example.com
       hosts:
         - host: dapp.example.com
   ```

3. **Network policies**

   Enable NetworkPolicy resources to restrict pod-to-pod traffic:

   ```yaml
   # Many subcharts include networkPolicy settings
   erpc:
     networkPolicy:
       enabled: true
       ingress:
         - from:
             - podSelector:
                 matchLabels:
                   app: dapp
   ```

4. **Pod security standards**

   Set securityContext for restricted PSS compliance:

   ```yaml
   dapp:
     podSecurityContext:
       runAsNonRoot: true
       runAsUser: 1001
       fsGroup: 1001
       seccompProfile:
         type: RuntimeDefault

     securityContext:
       allowPrivilegeEscalation: false
       capabilities:
         drop:
           - ALL
   ```

### High availability

1. **Increase replicas**

   ```yaml
   # Multiple RPC nodes for redundancy
   network:
     network-nodes:
       rpcReplicaCount: 3

   # DApp horizontal scaling
   dapp:
     replicaCount: 3

   # PostgreSQL replication (requires external operator)
   support:
     postgresql:
       architecture: replication
       replicaCount: 3
   ```

2. **Pod disruption budgets**

   Most subcharts include PodDisruptionBudget resources enabled by default.

3. **Spread across nodes**

   Use pod anti-affinity:

   ```yaml
   dapp:
     affinity:
       podAntiAffinity:
         preferredDuringSchedulingIgnoredDuringExecution:
           - weight: 100
             podAffinityTerm:
               labelSelector:
                 matchLabels:
                   app.kubernetes.io/name: dapp
               topologyKey: kubernetes.io/hostname
   ```

### Monitoring and observability (Production Advantage)

**ATK includes a comprehensive observability stack that provides
production-grade monitoring out of the box.** This is a competitive
differentiator—most tokenization platforms require you to assemble monitoring
yourself from multiple vendors. ATK gives you everything integrated and
pre-configured.

#### Accessing the observability stack

The observability stack is enabled by default. Access Grafana at the configured
hostname (default: `grafana.k8s.orb.local`):

```bash
# Get Grafana URL
kubectl get ingress -n atk grafana

# Default credentials (change immediately in production)
Username: settlemint
Password: atk
```

#### Pre-built dashboards for instant insights

ATK ships with production-ready dashboards covering every layer of the stack:

**Blockchain layer:**

- **Besu Dashboard**: Node sync status, transaction throughput, block production
  rates, gas usage, peer connections, and chain reorganizations
- **Portal Metrics**: RPC request rates, authentication flows, rate limiting,
  and gateway health

**Indexing layer:**

- **TheGraph Indexing Status**: Subgraph sync progress, indexing latency,
  deployment health, and entity counts
- **TheGraph Query Performance**: Query response times, cache hit rates, GraphQL
  operation metrics, and query complexity analysis

**RPC gateway:**

- **ERPC Dashboard**: Comprehensive metrics including request routing, upstream
  health, cache effectiveness, rate limiting, error rates, and latency
  percentiles across all configured RPC endpoints

**Storage layer:**

- **IPFS Dashboard**: Pin operations, bandwidth usage, peer connectivity, and
  content retrieval metrics

**Infrastructure:**

- **Kubernetes Overview**: Cluster resource utilization, pod health, node
  capacity, and namespace quotas
- **NGINX Ingress**: Request rates, response times, error rates, and upstream
  connectivity
- **Node Exporter**: System-level metrics (CPU, memory, disk, network) for each
  Kubernetes node
- **VictoriaMetrics**: TSDB metrics including series cardinality, ingestion
  rates, and storage usage
- **Tempo Tracing**: Distributed trace visualization with service dependency
  graphs

#### Monitoring stack components

**VictoriaMetrics** (Metrics storage and querying):

- Time-series database optimized for Prometheus metrics
- 1-month retention by default (configurable via
  `observability.victoria-metrics-single.server.retentionPeriod`)
- Query interface compatible with PromQL
- Remote write endpoint for external Prometheus instances

**Loki** (Log aggregation):

- Centralized logging for all ATK components
- 7-day retention (168 hours) by default
- Structured metadata support for advanced queries
- Automatic log pattern detection
- Integrated with Grafana Explore for interactive log analysis

**Tempo** (Distributed tracing):

- OpenTelemetry-compatible trace collection
- 7-day retention by default
- Trace-to-log correlation (click a trace span to see related logs)
- Metrics generation from trace data

**Alloy** (Telemetry collection agent):

- Scrapes metrics from all annotated pods automatically
- Collects logs from Kubernetes pods
- Receives OpenTelemetry traces
- Supports forwarding to external observability platforms (Grafana Cloud,
  Datadog, etc.)

**Grafana** (Visualization and dashboards):

- Pre-configured datasources for all telemetry backends
- Auto-loading of dashboards from ConfigMaps
- Dashboard version control through Kubernetes manifests
- Alert rule management with notification channels

#### Using observability for verification

**After deployment, verify correct operation:**

1. **Check TheGraph indexing status dashboard**:
   - Confirm subgraph sync is progressing
   - Validate no indexing errors in entity processing
   - Verify query performance meets SLAs (&lt;500ms p99)

2. **Monitor Besu dashboard**:
   - Ensure all validator nodes are producing blocks
   - Confirm transaction throughput matches expected load
   - Check peer connectivity (should match validator count)

3. **Review ERPC gateway metrics**:
   - Verify request routing to healthy upstreams
   - Confirm cache hit rate improves over time
   - Check error rate &lt;0.1% for production readiness

4. **Inspect application logs in Loki**:
   - Query: `{namespace="atk", app="dapp"} |= "error"`
   - Validate authentication flows completing successfully
   - Check for database connection pool saturation

#### Troubleshooting with dashboards

**Scenario: Slow transaction confirmations**

1. Open Besu dashboard → Check block production rate
2. If blocks are slow: Check validator connectivity and peer count
3. If blocks are normal: Open ERPC dashboard → Check RPC latency percentiles
4. Inspect Loki logs: `{namespace="atk"} |= "transaction" |= "timeout"`

**Scenario: Subgraph queries returning stale data**

1. TheGraph Indexing Status dashboard → Verify sync is not lagging
2. Check indexing error rate for failed block processing
3. Review TheGraph Query Performance → Confirm cache invalidation working
4. Inspect Loki: `{namespace="atk", app="graph-node"} |= "revert"`

**Scenario: High API latency**

1. Tempo traces → Identify slow spans in request path
2. Correlate trace IDs with logs to see detailed execution
3. Check database query performance in PostgreSQL metrics
4. Review Hasura dashboard for GraphQL query optimization needs

#### Configuring external observability platforms

Send telemetry to external platforms for long-term storage and advanced
analysis:

```yaml
# values-production.yaml
observability:
  alloy:
    endpoints:
      external:
        prometheus:
          enabled: true
          url: https://prometheus-external.example.com/api/v1/write
          basicAuth:
            username: atk-prod
            password: "${EXTERNAL_PROM_PASSWORD}"
        loki:
          enabled: true
          url: https://loki-external.example.com/loki/api/v1/push
          basicAuth:
            username: atk-prod
            password: "${EXTERNAL_LOKI_PASSWORD}"
        otel:
          enabled: true
          url: https://tempo-external.example.com:4318
```

This forwards all metrics, logs, and traces to external systems while keeping
local observability operational.

#### Alerting configuration

Configure alerting in Grafana to receive notifications on critical events:

1. Navigate to Alerting → Alert rules
2. Import pre-configured alert rules:
   - **High error rate**: Alert when error rate &gt;1% for 5 minutes
   - **Database connection exhaustion**: Alert when connection pool &gt;90% for
     3 minutes
   - **Blockchain sync lag**: Alert when Besu falls &gt;100 blocks behind
   - **Disk space critical**: Alert when PVC utilization &gt;85%
3. Configure notification channels (Slack, PagerDuty, email)

Example alert rule for transaction processing delays:

```yaml
# Alert when average block time exceeds 15 seconds
groups:
  - name: blockchain_health
    interval: 1m
    rules:
      - alert: SlowBlockProduction
        expr: rate(besu_block_height(5m)) < 0.067 # Less than 1 block per 15s
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Block production slower than expected"
          description:
            "Block production rate has fallen below 1 block per 15 seconds for
            10 minutes"
```

### Backup and disaster recovery

1. **Database backups**

   Use Velero or a PostgreSQL backup operator:

   ```bash
   # Example: Velero backup
   velero backup create atk-daily \
     --include-namespaces atk \
     --storage-location default \
     --snapshot-volumes
   ```

2. **Blockchain data snapshots**

   Besu data is stored in PersistentVolumes. Snapshot volumes periodically:

   ```bash
   # List PVCs for blockchain nodes
   kubectl get pvc -n atk | grep besu

   # Create volume snapshots via CSI driver
   kubectl create -f besu-snapshot.yaml
   ```

3. **Configuration backups**

   Store Helm values files in version control and backup:

   ```bash
   helm get values atk -n atk > atk-values-backup.yaml
   ```

## CI/CD integration

The kit includes GitHub Actions workflows for automated deployment.

### QA workflow

The `.github/workflows/qa.yml` workflow performs:

1. **Artifact generation** (`bun run artifacts`):
   - Compiles smart contracts
   - Generates ABIs and TypeScript bindings
   - Creates genesis allocation file

2. **Backend services startup** (`bun dev:up`):
   - Launches Docker Compose stack
   - Starts local PostgreSQL, Redis, Besu node

3. **Test execution** (`bunx turbo run ci:gha`):
   - Unit tests (Vitest, Foundry)
   - Integration tests (contract interactions)
   - E2E tests (Playwright)
   - Linting and type checking

4. **Chart testing** (conditional):
   - Helm chart linting (`ct lint`)
   - Deployment to ephemeral Kubernetes cluster (`ct install`)
   - Validates all resources reach ready state

5. **Docker image builds**:
   - DApp image: `Dockerfile.dapp`
   - Contract artifacts: `Dockerfile.contracts`
   - Pushed to GitHub Container Registry

### Deployment workflow

For production deployments, extend the QA workflow:

```yaml
# .github/workflows/deploy-production.yml
name: Deploy to Production

on:
  push:
    tags:
      - "v*"

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig

      - name: Deploy with Helm
        working-directory: kit/charts/atk
        run: |
          helm upgrade atk . \
            --install \
            --namespace atk \
            --create-namespace \
            --values values-production.yaml \
            --set dapp.image.tag=${{ github.ref_name }} \
            --timeout 20m \
            --wait
```

### GitOps approach

For declarative deployments, use ArgoCD or Flux:

**ArgoCD Application manifest**:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: atk
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/asset-tokenization-kit
    targetRevision: main
    path: kit/charts/atk
    helm:
      valueFiles:
        - values-production.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: atk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

## Troubleshooting

**ATK's integrated observability stack accelerates troubleshooting by providing
correlated metrics, logs, and traces in one interface.** Start with Grafana
dashboards to identify issues, then drill into detailed logs and traces.

### General troubleshooting workflow

1. **Check Grafana dashboards** for high-level health indicators
2. **Query Loki** for detailed application logs
3. **Inspect Tempo traces** to identify slow operations
4. **Use kubectl** for immediate remediation

### Pod crashes or restarts

**Using observability stack:**

1. **Open Grafana → Explore → Loki**:

   ```text
   # Find crash logs
   {namespace="atk"} |= "panic" or "fatal" or "OOM"

   # Check specific pod
   {namespace="atk", pod=~"dapp-.*"} |= "error"
   ```

2. **Check Kubernetes dashboard** in Grafana:
   - Identify pods with high restart counts
   - Review memory usage trends before OOM kills
   - Check CPU throttling that might cause timeouts

**Using kubectl:**

1. **Check pod logs**:

   ```bash
   kubectl logs -n atk <pod-name> --previous
   ```

2. **Describe pod for events**:

   ```bash
   kubectl describe pod -n atk <pod-name>
   ```

3. **Common issues identified via dashboards**:
   - **Database connection errors**: PostgreSQL dashboard shows connection pool
     exhaustion
   - **RPC connection errors**: ERPC dashboard displays all upstreams as
     unhealthy
   - **OOM kills**: Node Exporter dashboard shows memory saturation; increase
     limits in `values.yaml`

### Ingress not accessible

**Using observability stack:**

1. **Open NGINX Ingress dashboard** in Grafana:
   - Check request rate and error codes (4xx, 5xx)
   - Verify upstream connectivity to backend services
   - Review SSL certificate expiration warnings

2. **Query logs for connection errors**:

   ```text
   {namespace="atk", app="ingress-nginx"} |= "error" | json
   ```

**Using kubectl:**

1. **Verify ingress controller**:

   ```bash
   kubectl get pods -n atk -l app.kubernetes.io/name=ingress-nginx
   ```

2. **Check ingress resources**:

   ```bash
   kubectl get ingress -n atk
   ```

   Ensure each ingress has an `ADDRESS` assigned.

3. **DNS resolution**:

   ```bash
   nslookup dapp.example.com
   ```

   Verify DNS points to ingress controller LoadBalancer IP.

### Database migration failures

The DApp includes a post-install job that runs Drizzle migrations. If it fails:

1. **Check job logs**:

   ```bash
   kubectl logs -n atk job/dapp-migrations
   ```

2. **Manually run migrations**:

   ```bash
   kubectl exec -n atk deployment/dapp -- bun run db:migrate
   ```

### Genesis file regeneration

If you modify smart contract deployments, regenerate genesis allocations:

```bash
# Locally
bun run artifacts

# The network-bootstrapper job automatically compiles genesis.json
# Restart Besu nodes to pick up changes:
kubectl rollout restart statefulset/besu-validator -n atk
kubectl rollout restart statefulset/besu-rpc -n atk
```

### Helm upgrade failures

If `helm upgrade` fails mid-deployment:

1. **Check release status**:

   ```bash
   helm status atk -n atk
   ```

2. **Rollback to previous version**:

   ```bash
   helm rollback atk -n atk
   ```

3. **Force upgrade** (use cautiously):

   ```bash
   helm upgrade atk . --namespace atk --force
   ```

## Uninstallation

To completely remove the deployment:

```bash
# Uninstall Helm release
helm uninstall atk -n atk

# Delete namespace (removes all resources)
kubectl delete namespace atk
```

**Warning**: This deletes all data including blockchain state, databases, and
logs. Backup critical data before uninstalling.

## Next steps

- Review
  [Installation & configuration](/docs/developer-guides/deployment-ops/installation-configuration)
  for initial setup and configuration reference
- See [Testing and QA](/docs/developer-guides/quality-assurance) for running
  tests against deployed environments
- Explore [Development FAQ](/docs/developer-guides/setup) for common deployment
  issues
- Consult [Code Structure](/docs/developer-guides/architecture) to understand
  the monorepo layout
- Review
  [Contract Reference](/docs/developer-guides/smart-contracts/contract-reference)
  for smart contract interfaces
