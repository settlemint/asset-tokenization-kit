---
title: Deployment guide
description:
  Deploy the Asset Tokenization Kit to Kubernetes using Helm charts, including
  production configuration and CI/CD integration
---

This guide covers deploying the Asset Tokenization Kit to Kubernetes
environments using the included Helm charts. The kit provides a comprehensive
umbrella chart that orchestrates all required infrastructure components.

## Overview

The deployment architecture consists of:

- **Blockchain network** (Hyperledger Besu validators and RPC nodes)
- **RPC gateway** (ERPC for load balancing and caching)
- **Indexing layer** (TheGraph node and Blockscout explorer)
- **Application layer** (DApp frontend, Portal IAM, Hasura GraphQL)
- **Support services** (PostgreSQL, Redis, MinIO, NGINX Ingress)
- **Observability stack** (Grafana, Loki, VictoriaMetrics, Tempo)

<Mermaid
  chart={`
graph TB
    subgraph "Ingress Layer"
        NGINX(NGINX Ingress)
    end

    subgraph "Application Layer"
        DAPP(DApp Frontend)
        PORTAL(Portal IAM)
        HASURA(Hasura GraphQL)
    end

    subgraph "Blockchain Layer"
        ERPC(ERPC Gateway)
        VALIDATORS(Besu Validators)
        RPC(Besu RPC Nodes)
    end

    subgraph "Indexing Layer"
        GRAPH(TheGraph Node)
        BLOCKSCOUT(Blockscout Explorer)
    end

    subgraph "Data Layer"
        PG((PostgreSQL))
        REDIS((Redis))
        MINIO((MinIO))
    end

    subgraph "Observability"
        GRAFANA(Grafana)
        LOKI(Loki)
        VM(VictoriaMetrics)
    end

    NGINX --> DAPP
    NGINX --> PORTAL
    NGINX --> HASURA
    NGINX --> ERPC
    NGINX --> GRAPH
    NGINX --> BLOCKSCOUT

    DAPP --> HASURA
    DAPP --> PORTAL
    DAPP --> ERPC

    ERPC --> VALIDATORS
    ERPC --> RPC

    GRAPH --> ERPC
    GRAPH --> PG
    BLOCKSCOUT --> ERPC
    BLOCKSCOUT --> PG

    HASURA --> PG
    HASURA --> REDIS
    PORTAL --> PG
    PORTAL --> REDIS

    DAPP --> MINIO

    GRAFANA --> VM
    GRAFANA --> LOKI

`} />

## Helm chart structure

The Asset Tokenization Kit uses an umbrella chart architecture located in
[`kit/charts/atk/`](https://github.com/settlemint/asset-tokenization-kit/tree/main/kit/charts/atk):

### Chart dependencies

The main chart
([`kit/charts/atk/Chart.yaml`](https://github.com/settlemint/asset-tokenization-kit/blob/main/kit/charts/atk/Chart.yaml))
orchestrates 11 dependent subcharts:

```yaml
dependencies:
  - name: support # Infrastructure (NGINX, Redis, PostgreSQL, MinIO)
  - name: observability # Metrics, logs, traces (Grafana, Loki, VictoriaMetrics)
  - name: network # Blockchain network (Besu nodes)
  - name: erpc # RPC gateway with caching
  - name: ipfs # IPFS cluster for distributed storage
  - name: blockscout # Blockchain explorer
  - name: graph-node # TheGraph indexing protocol
  - name: portal # Identity and access management
  - name: hasura # GraphQL engine for database
  - name: txsigner # Transaction signing service
  - name: dapp # Frontend application
```

Each subchart can be enabled/disabled via the `enabled` flag in `values.yaml`.

### Directory layout

```
kit/charts/atk/
├── Chart.yaml              # Chart metadata and dependencies
│   https://github.com/settlemint/asset-tokenization-kit/blob/main/kit/charts/atk/Chart.yaml
├── values.yaml             # Default configuration values
│   https://github.com/settlemint/asset-tokenization-kit/blob/main/kit/charts/atk/values.yaml
├── values-openshift.yaml   # OpenShift-specific overrides
├── templates/              # Kubernetes resource templates
│   ├── _helpers.tpl        # Template helpers
│   ├── _common-helpers.tpl # Shared helper functions
│   └── image-pull-secrets.yaml
└── charts/                 # Subchart definitions
    https://github.com/settlemint/asset-tokenization-kit/tree/main/kit/charts/atk/charts
```

## Prerequisites

Before deploying, ensure you have:

1. **Kubernetes cluster** (v1.27+)
   - Minimum 8 CPU cores, 32GB RAM for full deployment
   - Storage provisioner with dynamic PVC support
   - LoadBalancer or Ingress controller support

2. **Required tools**:
   - `kubectl` (v1.27+)
   - `helm` (v3.13+)
   - `bun` (for chart documentation generation)

3. **Container registry access**:
   - GitHub Container Registry (`ghcr.io`)
   - Docker Hub (`docker.io`)
   - Kubernetes Registry (`registry.k8s.io`)

4. **DNS configuration**:
   - Wildcard DNS or individual records for each service hostname
   - Default hostnames use `.k8s.orb.local` (customize for your environment)

## Installation steps

### 1. Build chart dependencies

Navigate to the charts directory and update dependencies:

```bash
cd kit/charts/atk
helm dependency update
```

This downloads all subchart dependencies into the `charts/` directory.

### 2. Configure values

Create a custom `values.yaml` file for your environment. Start by copying the
default:

```bash
cp values.yaml values-production.yaml
```

### 3. Customize configuration

Edit `values-production.yaml` to match your environment:

#### Update hostnames

Replace all `.k8s.orb.local` hostnames with your domain:

<Tabs items={["Development/Local", "Staging", "Production"]}>
<Tab value="Development/Local">

```yaml
# Use .k8s.orb.local (default) or .localhost
dapp:
  ingress:
    hosts:
      - host: dapp.k8s.orb.local

erpc:
  ingress:
    hostname: rpc.k8s.orb.local

blockscout:
  blockscout:
    ingress:
      hostname: explorer.k8s.orb.local
```

</Tab>
<Tab value="Staging">

```yaml
# Staging environment with subdomain
dapp:
  ingress:
    hosts:
      - host: dapp-staging.example.com

erpc:
  ingress:
    hostname: rpc-staging.example.com

blockscout:
  blockscout:
    ingress:
      hostname: explorer-staging.example.com

graph-node:
  ingress:
    hostname: graph-staging.example.com

hasura:
  ingress:
    hostName: hasura-staging.example.com

portal:
  ingress:
    hostname: portal-staging.example.com

observability:
  grafana:
    ingress:
      hosts:
        - grafana-staging.example.com
```

</Tab>
<Tab value="Production">

```yaml
# Production with custom domain
dapp:
  ingress:
    hosts:
      - host: dapp.example.com

erpc:
  ingress:
    hostname: rpc.example.com

blockscout:
  blockscout:
    ingress:
      hostname: explorer.example.com

graph-node:
  ingress:
    hostname: graph.example.com

hasura:
  ingress:
    hostName: hasura.example.com

portal:
  ingress:
    hostname: portal.example.com

observability:
  grafana:
    ingress:
      hosts:
        - grafana.example.com
```

</Tab>
</Tabs>

#### Update authentication URLs

The DApp requires the `BETTER_AUTH_URL` to match the ingress hostname:

```yaml
dapp:
  secretEnv:
    BETTER_AUTH_URL: "https://dapp.example.com"
```

#### Update database and storage passwords

**CRITICAL**: Change all default passwords before production deployment:

```yaml
global:
  datastores:
    default:
      redis:
        password: "YOUR_REDIS_PASSWORD"
      postgresql:
        password: "YOUR_PG_PASSWORD"

    portal:
      postgresql:
        password: "YOUR_PORTAL_DB_PASSWORD"

    txsigner:
      postgresql:
        password: "YOUR_TXSIGNER_DB_PASSWORD"

    graphNode:
      postgresql:
        password: "YOUR_GRAPH_DB_PASSWORD"

    blockscout:
      postgresql:
        password: "YOUR_BLOCKSCOUT_DB_PASSWORD"

    hasura:
      postgresql:
        password: "YOUR_HASURA_DB_PASSWORD"

# Redis auth
support:
  redis:
    auth:
      password: "YOUR_REDIS_PASSWORD"

# Grafana admin credentials
observability:
  grafana:
    adminUser: admin
    adminPassword: "YOUR_GRAFANA_PASSWORD"
```

#### Update transaction signer mnemonic

**CRITICAL**: Generate a new mnemonic for production:

```yaml
txsigner:
  config:
    mnemonic: "YOUR_PRODUCTION_MNEMONIC_HERE"
    derivationPath: "m/44'/60'/0'/0/0"
```

Use a secure mnemonic generator or BIP39 tool. **Never commit this to version
control.**

#### Configure resource limits

Adjust resource requests and limits based on your cluster capacity:

<Tabs items={["Development", "Staging", "Production"]}>
<Tab value="Development">

```yaml
# Minimal resources for local testing
network:
  network-nodes:
    validatorReplicaCount: 1
    rpcReplicaCount: 1
    resources:
      requests:
        cpu: "60m"
        memory: "512Mi"
      limits:
        cpu: "360m"
        memory: "1024Mi"

dapp:
  resources:
    requests:
      cpu: "100m"
      memory: "512Mi"
    limits:
      cpu: "500m"
      memory: "1024Mi"
```

</Tab>
<Tab value="Staging">

```yaml
# Moderate resources for testing
network:
  network-nodes:
    validatorReplicaCount: 2
    rpcReplicaCount: 2
    resources:
      requests:
        cpu: "200m"
        memory: "1024Mi"
      limits:
        cpu: "1000m"
        memory: "2048Mi"

dapp:
  resources:
    requests:
      cpu: "250m"
      memory: "1024Mi"
    limits:
      cpu: "2000m"
      memory: "2048Mi"
```

</Tab>
<Tab value="Production">

```yaml
# Full resources for production workloads
network:
  network-nodes:
    validatorReplicaCount: 4
    rpcReplicaCount: 3
    resources:
      requests:
        cpu: "500m"
        memory: "2048Mi"
      limits:
        cpu: "2000m"
        memory: "4096Mi"

dapp:
  resources:
    requests:
      cpu: "500m"
      memory: "2048Mi"
    limits:
      cpu: "4000m"
      memory: "4096Mi"
```

</Tab>
</Tabs>

See the [Resource summary](#resource-summary) section for default allocations.

#### Configure storage sizes

Adjust persistent volume sizes for your data retention requirements:

```yaml
# Blockchain data
network:
  network-nodes:
    persistence:
      size: 100Gi  # Scale based on expected chain growth

# Metrics retention
observability:
  victoria-metrics-single:
    server:
      persistentVolume:
        size: 50Gi

# Log retention
observability:
  loki:
    singleBinary:
      persistence:
        size: 50Gi
```

### 4. Deploy the chart

Install the chart with your custom values:

```bash
helm install atk . \
  --namespace atk \
  --create-namespace \
  --values values-production.yaml \
  --timeout 20m
```

Or upgrade an existing deployment:

```bash
helm upgrade atk . \
  --namespace atk \
  --values values-production.yaml \
  --timeout 20m
```

### 5. Verify deployment

Check pod status:

```bash
kubectl get pods -n atk
```

All pods should reach `Running` or `Completed` state. The deployment includes:

- **Init jobs**: `network-bootstrapper` (generates genesis file)
- **StatefulSets**: Besu nodes, Graph Node, PostgreSQL, Redis
- **Deployments**: DApp, Portal, Hasura, ERPC, Blockscout
- **DaemonSets**: Node exporters, log collectors

Check service endpoints:

```bash
kubectl get ingress -n atk
```

Verify each ingress has an external address assigned.

### 6. Access applications

Once deployed, access the services via configured hostnames:

- **DApp**: `https://dapp.example.com`
- **Blockchain Explorer**: `https://explorer.example.com`
- **Grafana Dashboards**: `https://grafana.example.com`
- **Hasura Console**: `https://hasura.example.com/console`
- **RPC Endpoint**: `https://rpc.example.com`

## Configuration reference

### Global settings

All subcharts inherit global configuration:

```yaml
global:
  # Blockchain network identity
  chainId: "53771311147"
  chainName: "ATK"

  # Labels applied to all resources
  labels:
    environment: production
    team: platform

  # Centralized datastore configuration
  datastores:
    # Shared default settings
    default:
      redis:
        host: "redis"
        port: 6379
        username: "default"
        password: "atk"
      postgresql:
        host: "postgresql"
        port: 5432
        username: "postgres"
        password: "atk"

    # Service-specific overrides
    portal:
      postgresql:
        database: "portal"
        username: "portal"
      redis:
        db: 4

    hasura:
      redis:
        cacheDb: 2
        rateLimitDb: 3
```

### Network configuration

Control blockchain network topology:

```yaml
network:
  enabled: true

  network-bootstrapper:
    settings:
      validators: 1 # Number of validator identities to generate

  network-nodes:
    validatorReplicaCount: 1 # Validator pods (consensus)
    rpcReplicaCount: 1 # RPC pods (queries)

    persistence:
      size: 20Gi # Per-node storage

    resources:
      requests:
        cpu: "60m"
        memory: "512Mi"
      limits:
        cpu: "360m"
        memory: "1024Mi"
```

### ERPC gateway configuration

Configure RPC load balancing and caching:

```yaml
erpc:
  enabled: true

  ingress:
    enabled: true
    ingressClassName: "atk-nginx"
    hostname: rpc.k8s.orb.local

  resources:
    requests:
      cpu: "60m"
      memory: "256Mi"
    limits:
      cpu: "360m"
      memory: "512Mi"
```

ERPC caches responses in Redis databases configured in `global.datastores.erpc`.

### DApp configuration

Frontend application settings:

```yaml
dapp:
  enabled: true

  image:
    repository: ghcr.io/settlemint/asset-tokenization-kit
    # tag: defaults to chart appVersion

  ingress:
    enabled: true
    hosts:
      - host: dapp.k8s.orb.local
        paths:
          - path: /
            pathType: ImplementationSpecific

  # Environment variables (stored as secrets)
  secretEnv:
    BETTER_AUTH_URL: "https://dapp.k8s.orb.local"
    SETTLEMINT_BLOCKSCOUT_UI_ENDPOINT: "https://explorer.k8s.orb.local/"
    SETTLEMINT_MINIO_ENDPOINT: "http://minio:9000"
    SETTLEMINT_MINIO_ACCESS_KEY: "console"
    SETTLEMINT_MINIO_SECRET_KEY: "console123"

  resources:
    requests:
      cpu: "100m"
      memory: "1024Mi"
    limits:
      cpu: "3000m"
      memory: "2048Mi"
```

### Selective component deployment

Disable components not required for your environment:

```yaml
# Minimal deployment (no observability, no IPFS)
observability:
  enabled: false

ipfs:
  enabled: false

# Keep core services
network:
  enabled: true
erpc:
  enabled: true
graph-node:
  enabled: true
hasura:
  enabled: true
dapp:
  enabled: true
support:
  enabled: true
```

## Production best practices

### Security hardening

1. **Use secrets management**

   Replace inline secrets with Kubernetes Secret references:

   ```yaml
   global:
     datastores:
       default:
         postgresql:
           existingSecret: "atk-postgres-secret"
           existingSecretKeys:
             password: password
   ```

   Create the secret separately:

   ```bash
   kubectl create secret generic atk-postgres-secret \
     --from-literal=password='YOUR_SECURE_PASSWORD' \
     -n atk
   ```

2. **Enable TLS termination**

   Configure ingress with TLS certificates:

   ```yaml
   dapp:
     ingress:
       enabled: true
       ingressClassName: "nginx"
       tls:
         - secretName: dapp-tls
           hosts:
             - dapp.example.com
       hosts:
         - host: dapp.example.com
   ```

3. **Network policies**

   Enable NetworkPolicy resources to restrict pod-to-pod traffic:

   ```yaml
   # Many subcharts include networkPolicy settings
   erpc:
     networkPolicy:
       enabled: true
       ingress:
         - from:
             - podSelector:
                 matchLabels:
                   app: dapp
   ```

4. **Pod security standards**

   Set securityContext for restricted PSS compliance:

   ```yaml
   dapp:
     podSecurityContext:
       runAsNonRoot: true
       runAsUser: 1001
       fsGroup: 1001
       seccompProfile:
         type: RuntimeDefault

     securityContext:
       allowPrivilegeEscalation: false
       capabilities:
         drop:
           - ALL
   ```

### High availability

1. **Increase replicas**

   ```yaml
   # Multiple RPC nodes for redundancy
   network:
     network-nodes:
       rpcReplicaCount: 3

   # DApp horizontal scaling
   dapp:
     replicaCount: 3

   # PostgreSQL replication (requires external operator)
   support:
     postgresql:
       architecture: replication
       replicaCount: 3
   ```

2. **Pod disruption budgets**

   Most subcharts include PodDisruptionBudget resources enabled by default.

3. **Spread across nodes**

   Use pod anti-affinity:

   ```yaml
   dapp:
     affinity:
       podAntiAffinity:
         preferredDuringSchedulingIgnoredDuringExecution:
           - weight: 100
             podAffinityTerm:
               labelSelector:
                 matchLabels:
                   app.kubernetes.io/name: dapp
               topologyKey: kubernetes.io/hostname
   ```

### Monitoring and observability (Production Advantage)

**ATK includes a comprehensive observability stack that provides
production-grade monitoring out of the box.** This is a competitive
differentiator—most tokenization platforms require you to assemble monitoring
yourself from multiple vendors. ATK gives you everything integrated and
pre-configured.

#### Accessing the observability stack

The observability stack is enabled by default. Access Grafana at the configured
hostname (default: `grafana.k8s.orb.local`):

```bash
# Get Grafana URL
kubectl get ingress -n atk grafana

# Default credentials (change immediately in production)
Username: settlemint
Password: atk
```

#### Pre-built dashboards for instant insights

ATK ships with production-ready dashboards covering every layer of the stack:

**Blockchain layer:**

- **Besu Dashboard**: Node sync status, transaction throughput, block production
  rates, gas usage, peer connections, and chain reorganizations
- **Portal Metrics**: RPC request rates, authentication flows, rate limiting,
  and gateway health

**Indexing layer:**

- **TheGraph Indexing Status**: Subgraph sync progress, indexing latency,
  deployment health, and entity counts
- **TheGraph Query Performance**: Query response times, cache hit rates, GraphQL
  operation metrics, and query complexity analysis

**RPC gateway:**

- **ERPC Dashboard**: Comprehensive metrics including request routing, upstream
  health, cache effectiveness, rate limiting, error rates, and latency
  percentiles across all configured RPC endpoints

**Storage layer:**

- **IPFS Dashboard**: Pin operations, bandwidth usage, peer connectivity, and
  content retrieval metrics

**Infrastructure:**

- **Kubernetes Overview**: Cluster resource utilization, pod health, node
  capacity, and namespace quotas
- **NGINX Ingress**: Request rates, response times, error rates, and upstream
  connectivity
- **Node Exporter**: System-level metrics (CPU, memory, disk, network) for each
  Kubernetes node
- **VictoriaMetrics**: TSDB metrics including series cardinality, ingestion
  rates, and storage usage
- **Tempo Tracing**: Distributed trace visualization with service dependency
  graphs

#### Monitoring stack components

**VictoriaMetrics** (Metrics storage and querying):

- Time-series database optimized for Prometheus metrics
- 1-month retention by default (configurable via
  `observability.victoria-metrics-single.server.retentionPeriod`)
- Query interface compatible with PromQL
- Remote write endpoint for external Prometheus instances

**Loki** (Log aggregation):

- Centralized logging for all ATK components
- 7-day retention (168 hours) by default
- Structured metadata support for advanced queries
- Automatic log pattern detection
- Integrated with Grafana Explore for interactive log analysis

**Tempo** (Distributed tracing):

- OpenTelemetry-compatible trace collection
- 7-day retention by default
- Trace-to-log correlation (click a trace span to see related logs)
- Metrics generation from trace data

**Alloy** (Telemetry collection agent):

- Scrapes metrics from all annotated pods automatically
- Collects logs from Kubernetes pods
- Receives OpenTelemetry traces
- Supports forwarding to external observability platforms (Grafana Cloud,
  Datadog, etc.)

**Grafana** (Visualization and dashboards):

- Pre-configured datasources for all telemetry backends
- Auto-loading of dashboards from ConfigMaps
- Dashboard version control through Kubernetes manifests
- Alert rule management with notification channels

#### Using observability for verification

**After deployment, verify correct operation:**

1. **Check TheGraph indexing status dashboard**:
   - Confirm subgraph sync is progressing
   - Validate no indexing errors in entity processing
   - Verify query performance meets SLAs (&lt;500ms p99)

2. **Monitor Besu dashboard**:
   - Ensure all validator nodes are producing blocks
   - Confirm transaction throughput matches expected load
   - Check peer connectivity (should match validator count)

3. **Review ERPC gateway metrics**:
   - Verify request routing to healthy upstreams
   - Confirm cache hit rate improves over time
   - Check error rate &lt;0.1% for production readiness

4. **Inspect application logs in Loki**:
   - Query: `{namespace="atk", app="dapp"} |= "error"`
   - Validate authentication flows completing successfully
   - Check for database connection pool saturation

#### Troubleshooting with dashboards

**Scenario: Slow transaction confirmations**

1. Open Besu dashboard → Check block production rate
2. If blocks are slow: Check validator connectivity and peer count
3. If blocks are normal: Open ERPC dashboard → Check RPC latency percentiles
4. Inspect Loki logs: `{namespace="atk"} |= "transaction" |= "timeout"`

**Scenario: Subgraph queries returning stale data**

1. TheGraph Indexing Status dashboard → Verify sync is not lagging
2. Check indexing error rate for failed block processing
3. Review TheGraph Query Performance → Confirm cache invalidation working
4. Inspect Loki: `{namespace="atk", app="graph-node"} |= "revert"`

**Scenario: High API latency**

1. Tempo traces → Identify slow spans in request path
2. Correlate trace IDs with logs to see detailed execution
3. Check database query performance in PostgreSQL metrics
4. Review Hasura dashboard for GraphQL query optimization needs

#### Configuring external observability platforms

Send telemetry to external platforms for long-term storage and advanced
analysis:

```yaml
# values-production.yaml
observability:
  alloy:
    endpoints:
      external:
        prometheus:
          enabled: true
          url: https://prometheus-external.example.com/api/v1/write
          basicAuth:
            username: atk-prod
            password: "${EXTERNAL_PROM_PASSWORD}"
        loki:
          enabled: true
          url: https://loki-external.example.com/loki/api/v1/push
          basicAuth:
            username: atk-prod
            password: "${EXTERNAL_LOKI_PASSWORD}"
        otel:
          enabled: true
          url: https://tempo-external.example.com:4318
```

This forwards all metrics, logs, and traces to external systems while keeping
local observability operational.

#### Alerting configuration

Configure alerting in Grafana to receive notifications on critical events:

1. Navigate to Alerting → Alert rules
2. Import pre-configured alert rules:
   - **High error rate**: Alert when error rate &gt;1% for 5 minutes
   - **Database connection exhaustion**: Alert when connection pool &gt;90% for
     3 minutes
   - **Blockchain sync lag**: Alert when Besu falls &gt;100 blocks behind
   - **Disk space critical**: Alert when PVC utilization &gt;85%
3. Configure notification channels (Slack, PagerDuty, email)

Example alert rule for transaction processing delays:

```yaml
# Alert when average block time exceeds 15 seconds
groups:
  - name: blockchain_health
    interval: 1m
    rules:
      - alert: SlowBlockProduction
        expr: rate(besu_block_height(5m)) < 0.067 # Less than 1 block per 15s
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Block production slower than expected"
          description:
            "Block production rate has fallen below 1 block per 15 seconds for
            10 minutes"
```

### Backup and disaster recovery

1. **Database backups**

   Use Velero or a PostgreSQL backup operator:

   ```bash
   # Example: Velero backup
   velero backup create atk-daily \
     --include-namespaces atk \
     --storage-location default \
     --snapshot-volumes
   ```

2. **Blockchain data snapshots**

   Besu data is stored in PersistentVolumes. Snapshot volumes periodically:

   ```bash
   # List PVCs for blockchain nodes
   kubectl get pvc -n atk | grep besu

   # Create volume snapshots via CSI driver
   kubectl create -f besu-snapshot.yaml
   ```

3. **Configuration backups**

   Store Helm values files in version control and backup:

   ```bash
   helm get values atk -n atk > atk-values-backup.yaml
   ```

## Resource summary

Default resource allocations with full stack enabled:

| Component                             | Replicas | Request CPU      | Limit CPU         | Request Memory       | Limit Memory          | Storage           |
| ------------------------------------- | -------- | ---------------- | ----------------- | -------------------- | --------------------- | ----------------- |
| network.network-nodes                 | 2        | 60m (total 120m) | 360m (total 720m) | 512Mi (total 1024Mi) | 1024Mi (total 2048Mi) | 20Gi (total 40Gi) |
| erpc                                  | 1        | 60m              | 360m              | 256Mi                | 512Mi                 | -                 |
| blockscout.blockscout                 | 1        | 100m             | 600m              | 640Mi                | 1280Mi                | -                 |
| blockscout.frontend                   | 1        | 60m              | 360m              | 320Mi                | 640Mi                 | -                 |
| graph-node                            | 1        | 60m              | 360m              | 512Mi                | 1024Mi                | -                 |
| hasura                                | 1        | 80m              | 480m              | 384Mi                | 768Mi                 | -                 |
| portal                                | 1        | 60m              | 360m              | 256Mi                | 512Mi                 | -                 |
| txsigner                              | 1        | 60m              | 360m              | 192Mi                | 384Mi                 | -                 |
| dapp                                  | 1        | 100m             | 3000m             | 1024Mi               | 2048Mi                | -                 |
| support.ingress-nginx                 | 1        | 120m             | 720m              | 256Mi                | 512Mi                 | -                 |
| support.redis                         | 1        | 40m              | 240m              | 64Mi                 | 128Mi                 | 1Gi               |
| support.postgresql                    | 1        | -                | -                 | -                    | -                     | -                 |
| support.minio                         | 1        | 50m              | 300m              | 256Mi                | 512Mi                 | -                 |
| observability.grafana                 | 1        | 60m              | 360m              | 256Mi                | 512Mi                 | -                 |
| observability.loki                    | 1        | 200m             | 1200m             | 512Mi                | 1024Mi                | 10Gi              |
| observability.victoria-metrics-single | 1        | 60m              | 360m              | 256Mi                | 512Mi                 | 10Gi              |
| observability.alloy                   | 1        | 120m             | 720m              | 512Mi                | 1024Mi                | -                 |
| **Totals**                            | -        | **1.45 cores**   | **10.7 cores**    | **6.2Gi**            | **12.9Gi**            | **61Gi**          |

These totals represent the minimum cluster capacity required. Add overhead for
system components (kube-system, DNS, etc.).

## CI/CD integration

The kit includes GitHub Actions workflows for automated deployment.

### QA workflow

The `.github/workflows/qa.yml` workflow performs:

1. **Artifact generation** (`bun run artifacts`):
   - Compiles smart contracts
   - Generates ABIs and TypeScript bindings
   - Creates genesis allocation file

2. **Backend services startup** (`bun dev:up`):
   - Launches Docker Compose stack
   - Starts local PostgreSQL, Redis, Besu node

3. **Test execution** (`bunx turbo run ci:gha`):
   - Unit tests (Vitest, Foundry)
   - Integration tests (contract interactions)
   - E2E tests (Playwright)
   - Linting and type checking

4. **Chart testing** (conditional):
   - Helm chart linting (`ct lint`)
   - Deployment to ephemeral Kubernetes cluster (`ct install`)
   - Validates all resources reach ready state

5. **Docker image builds**:
   - DApp image: `Dockerfile.dapp`
   - Contract artifacts: `Dockerfile.contracts`
   - Pushed to GitHub Container Registry

### Deployment workflow

For production deployments, extend the QA workflow:

```yaml
# .github/workflows/deploy-production.yml
name: Deploy to Production

on:
  push:
    tags:
      - "v*"

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Configure kubectl
        run: |
          echo "${{ secrets.KUBE_CONFIG }}" | base64 -d > kubeconfig
          export KUBECONFIG=kubeconfig

      - name: Deploy with Helm
        working-directory: kit/charts/atk
        run: |
          helm upgrade atk . \
            --install \
            --namespace atk \
            --create-namespace \
            --values values-production.yaml \
            --set dapp.image.tag=${{ github.ref_name }} \
            --timeout 20m \
            --wait
```

### GitOps approach

For declarative deployments, use ArgoCD or Flux:

**ArgoCD Application manifest**:

```yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: atk
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/your-org/asset-tokenization-kit
    targetRevision: main
    path: kit/charts/atk
    helm:
      valueFiles:
        - values-production.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: atk
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
```

## Troubleshooting

**ATK's integrated observability stack accelerates troubleshooting by providing
correlated metrics, logs, and traces in one interface.** Start with Grafana
dashboards to identify issues, then drill into detailed logs and traces.

### General troubleshooting workflow

1. **Check Grafana dashboards** for high-level health indicators
2. **Query Loki** for detailed application logs
3. **Inspect Tempo traces** to identify slow operations
4. **Use kubectl** for immediate remediation

### Pod crashes or restarts

**Using observability stack:**

1. **Open Grafana → Explore → Loki**:

   ```text
   # Find crash logs
   {namespace="atk"} |= "panic" or "fatal" or "OOM"

   # Check specific pod
   {namespace="atk", pod=~"dapp-.*"} |= "error"
   ```

2. **Check Kubernetes dashboard** in Grafana:
   - Identify pods with high restart counts
   - Review memory usage trends before OOM kills
   - Check CPU throttling that might cause timeouts

**Using kubectl:**

1. **Check pod logs**:

   ```bash
   kubectl logs -n atk <pod-name> --previous
   ```

2. **Describe pod for events**:

   ```bash
   kubectl describe pod -n atk <pod-name>
   ```

3. **Common issues identified via dashboards**:
   - **Database connection errors**: PostgreSQL dashboard shows connection pool
     exhaustion
   - **RPC connection errors**: ERPC dashboard displays all upstreams as
     unhealthy
   - **OOM kills**: Node Exporter dashboard shows memory saturation; increase
     limits in `values.yaml`

### Ingress not accessible

**Using observability stack:**

1. **Open NGINX Ingress dashboard** in Grafana:
   - Check request rate and error codes (4xx, 5xx)
   - Verify upstream connectivity to backend services
   - Review SSL certificate expiration warnings

2. **Query logs for connection errors**:

   ```text
   {namespace="atk", app="ingress-nginx"} |= "error" | json
   ```

**Using kubectl:**

1. **Verify ingress controller**:

   ```bash
   kubectl get pods -n atk -l app.kubernetes.io/name=ingress-nginx
   ```

2. **Check ingress resources**:

   ```bash
   kubectl get ingress -n atk
   ```

   Ensure each ingress has an `ADDRESS` assigned.

3. **DNS resolution**:

   ```bash
   nslookup dapp.example.com
   ```

   Verify DNS points to ingress controller LoadBalancer IP.

### Database migration failures

The DApp includes a post-install job that runs Drizzle migrations. If it fails:

1. **Check job logs**:

   ```bash
   kubectl logs -n atk job/dapp-migrations
   ```

2. **Manually run migrations**:

   ```bash
   kubectl exec -n atk deployment/dapp -- bun run db:migrate
   ```

### Genesis file regeneration

If you modify smart contract deployments, regenerate genesis allocations:

```bash
# Locally
bun run artifacts

# The network-bootstrapper job automatically compiles genesis.json
# Restart Besu nodes to pick up changes:
kubectl rollout restart statefulset/besu-validator -n atk
kubectl rollout restart statefulset/besu-rpc -n atk
```

### Helm upgrade failures

If `helm upgrade` fails mid-deployment:

1. **Check release status**:

   ```bash
   helm status atk -n atk
   ```

2. **Rollback to previous version**:

   ```bash
   helm rollback atk -n atk
   ```

3. **Force upgrade** (use cautiously):

   ```bash
   helm upgrade atk . --namespace atk --force
   ```

## Uninstallation

To completely remove the deployment:

```bash
# Uninstall Helm release
helm uninstall atk -n atk

# Delete namespace (removes all resources)
kubectl delete namespace atk
```

**Warning**: This deletes all data including blockchain state, databases, and
logs. Backup critical data before uninstalling.

## Next steps

- Review [Testing and QA](./testing-qa) for running tests against deployed
  environments
- See [Development FAQ](./dev-faq) for common deployment issues
- Explore [Code Structure](./code-structure) to understand the monorepo layout
- Consult [Contract Reference](./contract-reference) for smart contract
  interfaces
