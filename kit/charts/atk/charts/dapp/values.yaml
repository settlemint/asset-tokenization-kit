# Default values for dapp.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: ghcr.io/settlemint/asset-tokenization-kit
  pullPolicy: IfNotPresent
  # Overrides the image tag whose default is the chart appVersion.
  tag: "2.0.0-main0d06815fc"

nameOverride: "dapp"
fullnameOverride: "dapp"

serviceAccount:
  # Specifies whether a service account should be created
  create: false # Set to true if your deployment needs specific permissions
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name: ""

# Additional annotations for the deployment metadata
annotations: {}
# Additional labels for the deployment pod metadata
podLabels: {}
podAnnotations: {}

podSecurityContext:
  fsGroup: 2016
  runAsNonRoot: true
  runAsUser: 2016

securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop:
      - ALL

service:
  type: ClusterIP
  port: 3000
  annotations: {}

ingress:
  enabled: false
  className: "atk-nginx"
  # Add other annotations as needed
  annotations: {}
  hosts:
    - host: dapp.local # Placeholder - Should be configured in parent chart values
      paths:
        - path: /((?:sm_|bpaas-)[^/]+)?/?(.*) # Match original path structure
          pathType: ImplementationSpecific
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

# Resource requests and limits
resources: {}
# Liveness and Readiness probes configuration
probes:
  liveness:
    initialDelaySeconds: 10
    periodSeconds: 15
    timeoutSeconds: 3
    failureThreshold: 10
    successThreshold: 1
  readiness:
    initialDelaySeconds: 5
    periodSeconds: 10
    timeoutSeconds: 3
    failureThreshold: 10
    successThreshold: 1

# Additional volumes for the deployment
volumes: []
#  - name: foo
#    secret:
#      secretName: mysecret
#      optional: false

# Additional volume mounts for the main container
volumeMounts: []
# - name: foo
#   mountPath: "/etc/foo"
#   readOnly: true

nodeSelector: {}

tolerations: []

affinity: {}

# NetworkPolicy configuration
networkPolicy:
  enabled: false
  # Ingress rules
  ingress:
    # Allow from ingress controller
    - from:
        - namespaceSelector:
            matchLabels:
              kubernetes.io/metadata.name: ingress-nginx
      ports:
        - protocol: TCP
          port: 3000
    # Allow from same namespace
    - from:
        - podSelector: {}
      ports:
        - protocol: TCP
          port: 3000

  # Egress rules
  egress:
    # Allow DNS resolution
    - to:
        - namespaceSelector: {}
          podSelector:
            matchLabels:
              k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53
    # Allow access to PostgreSQL
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: postgresql-ha
      ports:
        - protocol: TCP
          port: 5432
    # Allow access to Hasura GraphQL
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: graphql-engine
      ports:
        - protocol: TCP
          port: 8080
    # Allow access to Portal API
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: portal
      ports:
        - protocol: TCP
          port: 3000
        - protocol: TCP
          port: 3001
    # Allow access to ERPC
    - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/name: erpc
      ports:
        - protocol: TCP
          port: 4000
    # Allow external HTTPS (for external APIs)
    - to:
        - namespaceSelector: {}
      ports:
        - protocol: TCP
          port: 443

# Environment variables sourced from a secret
# These will be created in the secret.yaml template
secretEnv:
  BETTER_AUTH_URL: "https://dapp.local"
  NEXT_PUBLIC_APP_ID: "dapp"
  NEXTAUTH_URL: "https://dapp.local"
  OTEL_EXPORTER_OTLP_ENDPOINT: "http://o11y-alloy.btp-platform.svc.cluster.local:4318/v1/traces" # Adjust if needed
  OTEL_EXPORTER_OTLP_PROTOCOL: "http"
  SETTLEMINT_BLOCKSCOUT_UI_ENDPOINT: "https://explorer.local/" # Placeholder
  SETTLEMINT_HASURA_ADMIN_SECRET: "dummy-secret" # Placeholder
  SETTLEMINT_HASURA_DATABASE_URL: "postgresql://user:pass@host:port/db" # Placeholder
  SETTLEMINT_HASURA_ENDPOINT: "https://hasura.local/v1/graphql" # Placeholder
  SETTLEMINT_HD_PRIVATE_KEY: "dummy-key" # Placeholder
  SETTLEMINT_INSTANCE: "standalone"
  SETTLEMINT_PORTAL_GRAPHQL_ENDPOINT: "https://portal.local/graphql" # Placeholder
  SETTLEMINT_THEGRAPH_SUBGRAPHS_ENDPOINTS: '["https://graph.local/subgraphs/name/kit"]' # Placeholder

# Environment variables defined directly in the deployment
env: []
# - name: MY_VAR
#   value: my_value

# Autoscaling configuration (optional)
autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 10
  targetCPUUtilizationPercentage: 80
  # targetMemoryUtilizationPercentage: 80

# PodDisruptionBudget configuration
podDisruptionBudget:
  enabled: false
  # Minimum number of pods that must be available during disruption
  # Can be an integer or percentage (e.g., "50%")
  minAvailable: 1
  # Maximum number of pods that can be unavailable during disruption
  # Can be an integer or percentage (e.g., "50%")
  # Note: Only one of minAvailable or maxUnavailable can be specified
  # maxUnavailable: 1

# Init container configuration
initContainer:
  # Generic TCP check settings
  tcpCheck:
    enabled: true
    image:
      repository: ghcr.io/settlemint/btp-waitforit
      tag: v7.7.8
      pullPolicy: IfNotPresent
    timeout: 5 # Timeout in seconds for each dependency check
    dependencies:
      # Add internal Kubernetes service endpoints (service-name:port) for critical dependencies
      - name: postgres
        endpoint: "postgresql.atk.svc.cluster.local:5432"
      - name: hasura
        endpoint: "hasura.atk.svc.cluster.local:8080"
      - name: portal
        endpoint: "portal.atk.svc.cluster.local:3001"
      - name: graph-node-tcp # Renamed to distinguish from graphQLCheck
        endpoint: "graph-node-combined.atk.svc.cluster.local:8020" # Status API port (TCP check)
      - name: blockscout # Add Blockscout if needed, determine correct service/port
        endpoint: "blockscout-frontend-svc.atk.svc.cluster.local:80"




  # Specific check for GraphQL endpoint readiness (e.g., The Graph subgraph)
  graphQLCheck:
    enabled: true
    name: "wait-for-graphql"
    image:
      registry: docker.io
      repository: curlimages/curl
      tag: "8.15.0"
      pullPolicy: IfNotPresent
    # Wait for subgraph to finish syncing (extended timeout)
    url: "http://graph-node-combined.atk.svc.cluster.local:8000/subgraphs/name/kit"
    retries: 10
    retryDelaySeconds: 10
    # Use a more sophisticated query to check sync status
    query: "{ _meta { hasIndexingErrors block { number } } }"

job:
  enabled: true

  # Pod Security Context
  podSecurityContext: {}

  # Container Security Context
  securityContext: {}

  # Image configuration
  image:
    repository: docker.io/node
    tag: "23.11.1-slim"
    pullPolicy: IfNotPresent

  initContainer:
    hasuraCheck:
      image:
        repository: ghcr.io/settlemint/btp-waitforit
        tag: v7.7.8
        pullPolicy: IfNotPresent
      timeout: 5 # Timeout in seconds for each dependency check
      endpoint: hasura.atk.svc.cluster.local:8080
    cloneRepo:
      image:
        registry: docker.io
        repository: alpine/git
        tag: v2.49.1 # Or pin to a specific version
        pullPolicy: IfNotPresent

  # Pod annotations
  podAnnotations: {}

  # Image pull secrets
  imagePullSecrets: []

  # Resource configuration
  resources: {}

  workspace:
    # Size of the workspace PVC
    size: 1Gi
    # Storage class to use for the workspace PVC
    storageClass: ""