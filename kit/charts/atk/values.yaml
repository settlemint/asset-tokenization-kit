# Default values for atk.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

# Global values shared across all subcharts
global:
  labels:
    kots.io/app-slug: settlemint-atk
  # NetworkPolicy configuration
  networkPolicy:
    # Enable NetworkPolicies across all services
    enabled: false
  # Artifacts container configuration
  artifacts:
    # Image containing contract ABIs and genesis files
    image:
      registry: ghcr.io
      repository: settlemint/asset-tokenization-kit-artifacts
      tag: 2.0.0
      pullPolicy: IfNotPresent


besu-network:
  enabled: true
  # Besu network images
  besu-genesis:
    image:
      repository: ghcr.io/settlemint/quorum-genesis-tool
      tag: "sha-49c40f5"
      pullPolicy: IfNotPresent
    configServer:
      image:
        repository: docker.io/nginx
        tag: "1.29.1-alpine"
        pullPolicy: IfNotPresent
  besu-node:
    node:
      image:
        repository: docker.io/hyperledger/besu
        tag: "25.7.0"
        pullPolicy: IfNotPresent
    tessera:
      image:
        repository: docker.io/quorumengineering/tessera
        tag: "24.4"
        pullPolicy: IfNotPresent
    hooks:
      image:
        repository: ghcr.io/settlemint/quorum-genesis-tool
        tag: "sha-49c40f5"
        pullPolicy: IfNotPresent
    initContainers:
      testConnection:
        image:
          repository: docker.io/busybox
          tag: "1.37"
          pullPolicy: IfNotPresent
      checkConnection:
        image:
          repository: docker.io/curlimages/curl
          tag: "8.15.0"
          pullPolicy: IfNotPresent
  besu-validator-1:
    enabled: true
    storage:
      sizeLimit: "5Gi"
      pvcSizeLimit: "5Gi"
    resources: {}
  besu-validator-2:
    enabled: false
    storage:
      sizeLimit: "5Gi"
      pvcSizeLimit: "5Gi"
    resources: {}
  besu-validator-3:
    enabled: false
    storage:
      sizeLimit: "5Gi"
      pvcSizeLimit: "5Gi"
    resources: {}
  besu-validator-4:
    enabled: false
    storage:
      sizeLimit: "5Gi"
      pvcSizeLimit: "5Gi"
    resources: {}
  besu-rpc-1:
    enabled: true
    storage:
      sizeLimit: "5Gi"
      pvcSizeLimit: "5Gi"
    resources: {}
  besu-rpc-2:
    enabled: false
    storage:
      sizeLimit: "5Gi"
      pvcSizeLimit: "5Gi"
    resources: {}
  rawGenesisConfig:
    blockchain:
      nodes:
        count: 1

erpc:
  enabled: true
  image:
    registry: ghcr.io
    repository: erpc/erpc
    tag: "0.0.54"
    pullPolicy: IfNotPresent
  # Init containers
  initContainers:
    waitforit:
      image:
        repository: ghcr.io/settlemint/btp-waitforit
        tag: "v7.7.8"
        pullPolicy: IfNotPresent
  # Test container
  test:
    image:
      repository: docker.io/busybox
      tag: "1.37"
      pullPolicy: IfNotPresent
  resources: {}
  ingress:
    enabled: true
    className: "atk-nginx"
    hosts:
      - host: rpc.k8s.orb.local
        paths:
          - path: /
            pathType: ImplementationSpecific
  config:
    logLevel: info
    server:
      httpHostV4: 0.0.0.0
      httpPort: 4000
    projects:
      - id: settlemint
        upstreams:
          - id: besu-node-rpc-1
            endpoint: http://besu-node-rpc-1:8545
            evm:
              chainId: 1337
            failsafe:
              timeout:
                duration: 30s
          - id: besu-node-validator-1
            endpoint: http://besu-node-validator-1:8545
            evm:
              chainId: 1337
            failsafe:
              timeout:
                duration: 30s
    database:
      evmJsonRpcCache:
        # Define one or more storage connectors with unique IDs useful in policies
        connectors:
          - id: redis-cache
            driver: redis  # Refer to "redis" driver docs below
            redis:
              addr: redis.atk.svc.cluster.local:6379
              password: "atk"
              db: 0
              connPoolSize: 128
        policies:
          - network: "*"
            method: "*"
            finality: finalized
            connector: redis-cache
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "4001"
  podLabels:
    app.kubernetes.io/component: erpc


blockscout:
  enabled: true
  blockscout-stack:
    podAnnotations:
      prometheus.io/scrape: "true"
      prometheus.io/port: "4000"
      prometheus.io/path: "/metrics"
    blockscout:
      image:
        repository: ghcr.io/blockscout/blockscout
        tag: "9.0.2"
        pullPolicy: IfNotPresent
      # Add an extra init container to wait for PostgreSQL
      init:
        enabled: true
        command:
          - /bin/sh
        args:
          - -c
          - |  # Using YAML multiline string for clarity
            echo "Waiting for postgresql:5432..."
            while ! nc -z postgresql 5432; do
              sleep 2;
            done;
            echo "PostgreSQL is ready!"
            # Original command:
            bin/blockscout eval "Elixir.Explorer.ReleaseTasks.create_and_migrate()"
      ingress:
        hostname: explorer.k8s.orb.local
      env:
        API_URL: https://explorer.k8s.orb.local
        WEBAPP_URL: https://explorer.k8s.orb.local
      resources: {}
    frontend:
      ingress:
        hostname: explorer.k8s.orb.local
      image:
        repository: ghcr.io/blockscout/frontend
        tag: "v2.2.1"
        pullPolicy: IfNotPresent

graph-node:
  enabled: true
  image:
    repository: docker.io/graphprotocol/graph-node
    tag: "v0.40.0"
    pullPolicy: IfNotPresent
  initContainers:
    kubectlImage:
      repository: docker.io/kubesphere/kubectl
      tag: "v1.33.4"
      pullPolicy: IfNotPresent
    postgresImage:
      repository: docker.io/postgres
      tag: "17.6-alpine"
      pullPolicy: IfNotPresent
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8040"
    prometheus.io/path: "/metrics"

hasura:
  enabled: true
  graphql-engine:
    replicas: 1
    ingress:
      hostName: "hasura.k8s.orb.local"
    labels:
      app.kubernetes.io/instance: atk
      kots.io/app-slug: settlemint-atk
      app.kubernetes.io/component: hasura
    image:
      repository: docker.io/hasura/graphql-engine
      tag: v2.48.4
      ## Specify a imagePullPolicy
      ## ref: https://kubernetes.io/docs/user-guide/images/#pre-pulling-images
      ##
      pullPolicy: IfNotPresent
    # Init container for waiting for PostgreSQL
    initContainers:
      - name: wait-for-postgresql-ready
        image: docker.io/postgres:17.6-alpine
        command:
          - /bin/sh
          - -c
          - |
            set -e
            echo "Waiting for PostgreSQL to be ready..."

            # Add random delay to prevent all nodes from connecting simultaneously
            RANDOM_DELAY=$((RANDOM % 30 + 5))
            echo "Adding random delay of ${RANDOM_DELAY} seconds to stagger connections..."
            sleep $RANDOM_DELAY

            # Function to test PostgreSQL connection
            test_postgres() {
              pg_isready -h postgresql -p 5432 -U hasura && \
              psql -h postgresql -p 5432 -U hasura -d hasura -c "SELECT 1;" > /dev/null 2>&1
            }

            # Wait with exponential backoff
            RETRY_COUNT=0
            MAX_RETRIES=30
            WAIT_TIME=2

            while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
              if test_postgres; then
                echo "PostgreSQL is ready!"
                exit 0
              fi

              RETRY_COUNT=$((RETRY_COUNT + 1))
              echo "PostgreSQL not ready (attempt $RETRY_COUNT/$MAX_RETRIES). Waiting ${WAIT_TIME}s..."
              sleep $WAIT_TIME

              # Exponential backoff with max of 30 seconds
              WAIT_TIME=$((WAIT_TIME * 2))
              if [ $WAIT_TIME -gt 30 ]; then
                WAIT_TIME=30
              fi
            done

            echo "PostgreSQL failed to become ready after $MAX_RETRIES attempts"
            exit 1
        env:
          - name: PGPASSWORD
            value: "atk"

portal:
  enabled: true
  image:
    registry: ghcr.io
    repository: settlemint/btp-scs-portal
    tag: "8.6.3"
    pullPolicy: IfNotPresent
  podLabels:
    app.kubernetes.io/component: portal
  podAnnotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "3000"
    prometheus.io/path: "/portal-metrics"
  initContainers:
    - name: wait-for-postgresql-ready
      image: docker.io/postgres:17.6-alpine
      command:
        - /bin/sh
        - -c
        - |
          set -e
          echo "Waiting for PostgreSQL to be ready..."

          # Add random delay to prevent all nodes from connecting simultaneously
          RANDOM_DELAY=$((RANDOM % 30 + 5))
          echo "Adding random delay of ${RANDOM_DELAY} seconds to stagger connections..."
          sleep $RANDOM_DELAY

          # Function to test PostgreSQL connection
          test_postgres() {
            pg_isready -h postgresql -p 5432 -U portal && \
            psql -h postgresql -p 5432 -U portal -d portal -c "SELECT 1;" > /dev/null 2>&1
          }

          # Wait with exponential backoff
          RETRY_COUNT=0
          MAX_RETRIES=30
          WAIT_TIME=2

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if test_postgres; then
              echo "PostgreSQL is ready!"
              exit 0
            fi

            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "PostgreSQL not ready (attempt $RETRY_COUNT/$MAX_RETRIES). Waiting ${WAIT_TIME}s..."
            sleep $WAIT_TIME

            # Exponential backoff with max of 30 seconds
            WAIT_TIME=$((WAIT_TIME * 2))
            if [ $WAIT_TIME -gt 30 ]; then
              WAIT_TIME=30
            fi
          done

          echo "PostgreSQL failed to become ready after $MAX_RETRIES attempts"
          exit 1
      env:
        - name: PGPASSWORD
          value: "atk"

support:
  enabled: true

  # https://github.com/kubernetes/ingress-nginx/blob/main/charts/ingress-nginx/values.yaml
  ingress-nginx:
    replicaCount: 1
    controller:
      resources: {}
      image:
        repository: registry.k8s.io/ingress-nginx/controller
        tag: "v1.13.1"
        digest: ""

  # https://github.com/stakater/Reloader/tree/master/deployments/kubernetes/chart/reloader
  reloader:
    enabled: true
    image:
      repository: ghcr.io/stakater/reloader
      tag: v1.4.6

  minio:
    enabled: true
    image:
      repository: docker.io/minio/minio
      tag: RELEASE.2025-07-23T15-54-02Z
      pullPolicy: IfNotPresent
    mcImage:
      repository: docker.io/minio/minio
      tag: RELEASE.2025-07-23T15-54-02Z
      pullPolicy: IfNotPresent
    ingress:
      enabled: true
      ingressClassName: atk-nginx
      path: /
      hosts:
        - minio.k8s.orb.local

  # Redis configuration
  redis:
    enabled: true
    image:
      # -- Redis image registry
      registry: docker.io
      # -- Redis image repository
      repository: redis
      # -- Redis image tag
      tag: "8.2.1-alpine"
    fullnameOverride: redis
    commonLabels:
      kots.io/app-slug: settlemint-atk
      app.kubernetes.io/managed-by: helm
    auth:
      enabled: true
      password: "atk"
    persistence:
      enabled: true
      size: 1Gi
    resources:
      requests:
        memory: "128Mi"
        cpu: "100m"
      limits:
        memory: "256Mi"
        cpu: "200m"

  postgresql:
    image:
      registry: docker.io
      repository: postgres
      tag: "17.6-alpine"

observability:
  enabled: true

  # https://github.com/kubernetes-sigs/metrics-server/blob/master/charts/metrics-server/values.yaml
  metrics-server:
    enabled: false
    resources: {}
    image:
      repository: registry.k8s.io/metrics-server/metrics-server
      tag: "v0.8.0"

  # https://github.com/prometheus-community/helm-charts/blob/main/charts/kube-state-metrics/values.yaml
  kube-state-metrics:
    resources: {}
    image:
      registry: registry.k8s.io
      repository: kube-state-metrics/kube-state-metrics
      # If unset use v + .Charts.appVersion
      tag: "v2.16.0"

  # https://github.com/VictoriaMetrics/helm-charts/blob/master/charts/victoria-metrics-single/values.yaml
  victoria-metrics-single:
    server:
      persistentVolume:
        storageClass: ""
        size: 10Gi
      resources: {}
      image:
        registry: docker.io
        repository: victoriametrics/victoria-metrics
        tag: "v1.124.0"

  # https://github.com/grafana/loki/blob/main/production/helm/loki/values.yaml
  loki:
    loki:
      image:
        # -- The Docker registry
        registry: docker.io
        # -- Docker image repository
        repository: grafana/loki
        # -- Overrides the image tag whose default is the chart's appVersion
        tag: 3.5.3
    singleBinary:
      persistence:
        size: 10Gi
      resources: {}
      extraEnv: {}
        # Keep a little bit lower than memory limits
        # - name: GOMEMLIMIT
        #   value: 3750MiB
    memcached:
      # -- Enable the built in memcached server provided by the chart
      enabled: true
      image:
        # -- Memcached Docker image repository
        repository: docker.io/memcached
        # -- Memcached Docker image tag
        tag: 1.6.39-alpine
    memcachedExporter:
      image:
        repository: docker.io/prom/memcached-exporter
        tag: v0.15.3
    sidecar:
      image:
        # -- The Docker registry and image for the k8s sidecar
        repository: docker.io/kiwigrid/k8s-sidecar
        tag: 1.30.9
    gateway:
      image:
        registry: docker.io
        repository: nginxinc/nginx-unprivileged
        tag: "1.29-alpine"

  # https://github.com/grafana/alloy/blob/main/operations/helm/charts/alloy/values.yaml
  alloy:
    alloy:
      resources: {}
    image:
      registry: "docker.io"
      # -- Grafana Alloy image repository.
      repository: grafana/alloy
      tag: v1.10.2
    configReloader:
      image:
        registry: "quay.io"
        # -- Repository to get config reloader image from.
        repository: prometheus-operator/prometheus-config-reloader
        # -- Tag of image to use for config reloading.
        tag: v0.84.1

  grafana:
    image:
      # -- The Docker registry
      registry: docker.io
      # -- Docker image repository
      repository: grafana/grafana
      # Overrides the Grafana image tag whose default is the chart appVersion
      tag: "12.1.1"
    # Sidecar configuration for Grafana dashboards and datasources
    sidecar:
      image:
        registry: docker.io
        repository: kiwigrid/k8s-sidecar
        tag: "1.30.9"
    ingress:
      hosts:
        - grafana.k8s.orb.local
    adminUser: settlemint
    adminPassword: atk

  tempo:
    tempo:
      repository: docker.io/grafana/tempo
      tag: "2.8.1"
    tempoQuery:
      repository: docker.io/grafana/tempo-query
      tag: "2.8.1"
    server:
      resources: {}

  prometheus-node-exporter:
    image:
      registry: quay.io
      repository: prometheus/node-exporter
      # Overrides the image tag whose default is {{ printf "v%s" .Chart.AppVersion }}
      tag: "v1.9.1"

# Transaction Signer Configuration
txsigner:
  enabled: true
  image:
    registry: ghcr.io
    repository: settlemint/btp-signer
    tag: "7.15.2"
    pullPolicy: IfNotPresent
  # Init container for testing
  test:
    image:
      repository: docker.io/busybox
      tag: "1.37"
      pullPolicy: IfNotPresent
  # Configuration for the txsigner subchart
  config:
    mnemonic: "gate yellow grunt wrestle disease obtain mixed nature mansion tape purchase awful"
    derivationPath: "m/44'/60'/0'/0/0"
  postgresql: postgresql://txsigner:atk@postgresql:5432/txsigner?sslmode=disable
  replicaCount: 1
  resources: {}

# DApp Frontend Configuration
dapp:
  enabled: true
  image:
    repository: ghcr.io/settlemint/asset-tokenization-kit
    tag: "2.0.0-main71f0929ab"
    pullPolicy: IfNotPresent
  replicaCount: 1
  ingress:
    enabled: true
    # IMPORTANT: Set the correct hostname for your environment
    hosts:
      - host: dapp.k8s.orb.local  # Example hostname
        paths:
          - path: /  # Adjust path if needed
            pathType: ImplementationSpecific
  resources: {}
  secretEnv:
    BETTER_AUTH_URL: "https://dapp.k8s.orb.local"
    NEXT_PUBLIC_APP_ID: "dapp"
    NEXTAUTH_URL: "https://dapp.k8s.orb.local"
    OTEL_EXPORTER_OTLP_ENDPOINT: "http://alloy:4318/v1/traces"  # Adjust if needed
    OTEL_EXPORTER_OTLP_PROTOCOL: "http"
    SETTLEMINT_BLOCKSCOUT_UI_ENDPOINT: "http://blockscout-frontend-svc/"
    SETTLEMINT_HASURA_ADMIN_SECRET: "atk"
    SETTLEMINT_HASURA_DATABASE_URL: "postgresql://hasura:atk@postgresql:5432/hasura"
    SETTLEMINT_HASURA_ENDPOINT: "http://hasura:8080/v1/graphql"
    SETTLEMINT_HD_PRIVATE_KEY: "atk-hd-private-key"
    SETTLEMINT_INSTANCE: "standalone"
    SETTLEMINT_PORTAL_GRAPHQL_ENDPOINT: "http://portal:3001/graphql"
    SETTLEMINT_THEGRAPH_SUBGRAPHS_ENDPOINTS: '["http://graph-node-combined:8000/subgraphs/name/kit"]'
  initContainer:
    # Generic TCP check settings
    tcpCheck:
      enabled: true
      timeout: 0  # Timeout in seconds for each dependency check
      image:
        repository: ghcr.io/settlemint/btp-waitforit
        tag: v7.7.8
        pullPolicy: IfNotPresent
      dependencies:
        # Add internal Kubernetes service endpoints (service-name:port) for critical dependencies
        - name: postgres
          endpoint: "postgresql:5432"
        - name: hasura
          endpoint: "hasura:8080"
        - name: portal
          endpoint: "portal:3001"
        - name: graph-node-tcp  # Renamed to distinguish from graphQLCheck
          endpoint: "graph-node-combined:8020"  # Status API port (TCP check)
        - name: blockscout  # Add Blockscout if needed, determine correct service/port
          endpoint: "blockscout-frontend-svc:80"

    # Specific check for GraphQL endpoint readiness (e.g., The Graph subgraph)
    graphQLCheck:
      enabled: true
      name: wait-for-graph-subgraph-kit  # Name for the init container
      image:
        registry: docker.io
        repository: curlimages/curl
        tag: "8.15.0"
        pullPolicy: IfNotPresent
      # Target GraphQL endpoint URL (where POST requests are sent)
      url: "http://graph-node-combined:8000/subgraphs/name/kit"
      query: '{ __typename }'  # Basic query to check for GraphQL errors
      retryDelaySeconds: 20
      timeoutSeconds: 10  # Max time per curl attempt
      connectTimeoutSeconds: 5  # Max time to connect
      retries: 24
  podLabels:
    app.kubernetes.io/component: dapp
    kots.io/app-slug: settlemint-atk
